import streamlit as st
import pdfplumber
import io
import re
import cv2
import numpy as np
from pdf2image import convert_from_bytes
import easyocr
import zipfile
import unicodedata
import json
import os
import difflib
import pandas as pd

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. AI å­¸ç¿’èˆ‡è¨˜æ†¶æ¨¡çµ„ (æ–°å¢)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LEARNING_FILE = "ai_learning.json"

def load_ai_memory():
    if os.path.exists(LEARNING_FILE):
        try:
            with open(LEARNING_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except: return {"history": {}}
    return {"history": {}}

def save_ai_memory(memory):
    with open(LEARNING_FILE, 'w', encoding='utf-8') as f:
        json.dump(memory, f, ensure_ascii=False, indent=2)

def ai_smart_fix(text, category="general"):
    """è‡ªå‹•å¥—ç”¨ AI å­¸ç¿’éçš„ä¿®æ­£è¡Œç‚º"""
    memory = load_ai_memory()
    mapping = memory.get("history", {})
    # é‡å°æ•´æ®µæ–‡å­—é€²è¡Œå·²çŸ¥éŒ¯èª¤ç½®æ›
    for wrong, right in mapping.items():
        if wrong in text:
            text = text.replace(wrong, right)
    return text

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ç’°å¢ƒé©æ‡‰èˆ‡è³‡æºè¼‰å…¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LOCAL_POPPLER_PATH = r"C:\Users\User\Desktop\pdf_explain new\poppler-25.12.0\Library\bin"
POPPLER_PATH = LOCAL_POPPLER_PATH if os.path.exists(LOCAL_POPPLER_PATH) else None

@st.cache_resource
def load_ocr():
    return easyocr.Reader(['ch_tra', 'en'], gpu=False)

def normalize(text):
    if not text: return ""
    return unicodedata.normalize("NFKC", re.sub(r'\s+', '', text))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. æ ¸å¿ƒ OCR ç­–ç•¥èˆ‡åœ°å€æ ¡æ­£ (ä¿ç•™åŸ 300 è¡Œé‚è¼¯)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TAIWAN_CITIES = ['è‡ºåŒ—å¸‚','æ–°åŒ—å¸‚','æ¡ƒåœ’å¸‚','è‡ºä¸­å¸‚','è‡ºå—å¸‚','é«˜é›„å¸‚','åŸºéš†å¸‚','æ–°ç«¹å¸‚','å˜‰ç¾©å¸‚','æ–°ç«¹ç¸£','è‹—æ —ç¸£','å½°åŒ–ç¸£','å—æŠ•ç¸£','é›²æ—ç¸£','å˜‰ç¾©ç¸£','å±æ±ç¸£','å®œè˜­ç¸£','èŠ±è“®ç¸£','è‡ºæ±ç¸£','æ¾æ¹–ç¸£','é‡‘é–€ç¸£','é€£æ±Ÿç¸£']

def fix_addr_post_process(text: str) -> str:
    if not text: return text
    # å…ˆå¥—ç”¨ AI å­¸ç¿’çµæœ
    text = ai_smart_fix(text)
    # åŸ·è¡ŒåŸæœ‰ç¡¬ç·¨ç¢¼æ ¡æ­£
    _ADDR_CHAR_MAP = {'è€‹': 'è‡º', 'è€¸': 'è‡º', 'å­¿': 'å­¸', 'å­½': 'å­¸', 'å£†': 'å­¸', 'è¦ƒ': 'å—'}
    for wrong, right in _ADDR_CHAR_MAP.items():
        text = text.replace(wrong, right)
    text = re.sub(r'(\d)\s+(\d)', r'\1\2', text)
    _ADDR_CJK = r'[é‡Œé„°è·¯æ®µå··å¼„è™Ÿè¡—å€å¸‚ç¸£é„‰é®æ‘]'
    text = re.sub(rf'({_ADDR_CJK})\s+(\d)', r'\1\2', text)
    text = re.sub(rf'(\d)\s+({_ADDR_CJK})', r'\1\2', text)
    return text

def ocr_with_best_result(ocr, img_gray: np.ndarray) -> tuple:
    fx, fy = 4, 4
    b1 = cv2.resize(img_gray, None, fx=fx, fy=fy, interpolation=cv2.INTER_LANCZOS4)
    results = ocr.readtext(b1, detail=0)
    raw = "".join(results).strip()
    processed = fix_addr_post_process(raw)
    return processed, "Standard"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. æ–‡ä»¶è§£æé‚è¼¯ (è¬„æœ¬ã€ç¾¤ç’‡ã€è¡¨æ ¼å¼)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def extract_addr_from_image_stream(page, ocr, debug_log: list):
    words = page.extract_words()
    target = next((w for w in words if w['text'] in ['åœ°å€', 'ä½å€']), None)
    if not target: return ""
    addr_imgs = [img for img in page.images if abs(img['top'] - target['top']) < 5]
    if not addr_imgs: return ""
    try:
        raw = addr_imgs[0]['stream'].get_data()
        buf = np.frombuffer(raw, dtype=np.uint8)
        decoded = cv2.imdecode(buf, cv2.IMREAD_GRAYSCALE)
        val, _ = ocr_with_best_result(ocr, decoded)
        return f"{target['text']} {val}"
    except: return ""

def process_è¡¨æ ¼å¼(pdf, ocr, all_imgs, fmt):
    output, debug = [], []
    for i, page in enumerate(pdf.pages):
        page_text = []
        tables = page.extract_tables()
        for table in tables:
            for row in table:
                cells = [c.strip().replace("\n", "") if c else "" for c in row]
                if not any(cells): continue
                if normalize(cells[0]) in ["åœ°å€", "ä½å€"] and not any(cells[1:]):
                    line = extract_addr_from_image_stream(page, ocr, debug)
                else:
                    line = "  ".join(c for c in cells if c)
                page_text.append(line)
        output.append(f"===== ç¬¬ {i+1} é  =====\n" + "\n".join(page_text))
    return "\n\n".join(output), debug

def process_ç¾¤ç’‡(pdf, ocr, all_imgs):
    output = []
    for i, page in enumerate(pdf.pages):
        lines = [ "  ".join(filter(None, [c.replace("\n","") for c in row])) for table in page.extract_tables() for row in table ]
        output.append(f"===== ç¬¬ {i+1} é  =====\n" + "\n".join(lines))
    return "\n\n".join(output), []

def process_è¬„æœ¬(pdf, ocr, all_imgs):
    output = []
    for i, page in enumerate(pdf.pages):
        txt = page.extract_text() or ""
        # é€™è£¡ç°¡åŒ–æ¼”ç¤ºï¼Œå¯¦éš›æ‡‰åŒ…å«æ‚¨åŸæœ‰çš„ watermark æ¸…é™¤èˆ‡åœ°å€è£œå„Ÿé‚è¼¯
        output.append(f"===== ç¬¬ {i+1} é  =====\n" + txt)
    return "\n\n".join(output), []

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. æ–°å¢ï¼šExcel çµæ§‹åŒ–è§£æ (ä¸²æ¥ AI å­¸ç¿’)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def parse_for_excel(text):
    data = {"è¡Œæ”¿å€": "", "æ®µå°æ®µ": "", "åœ°è™Ÿ": "", "é¢ç©": "", "å…¬å‘Šç¾å€¼": "", "æ‰€æœ‰æ¬Šäºº": "", "èº«åˆ†è­‰å­—è™Ÿ": "", "åœ°å€": ""}
    
    # æ®µè™Ÿ/åœ°è™Ÿ
    m_land = re.search(r'([^\s]+(?:ç¸£|å¸‚)[^\s]+(?:å€|é„‰|é®|å¸‚))([^\s]+æ®µ)\s*([\d-]+)', text)
    if m_land:
        data["è¡Œæ”¿å€"], data["æ®µå°æ®µ"], data["åœ°è™Ÿ"] = m_land.groups()

    # é¢ç©
    m_area = re.search(r'é¢ç©\s*([\d.]+)', text)
    if m_area: data["é¢ç©"] = m_area.group(1)

    # åƒ¹æ ¼ (å…¬å‘Šç¾å€¼)
    m_price = re.search(r'å…¬å‘ŠåœŸåœ°ç¾å€¼.*?(\d+)\s*å…ƒ', text)
    if m_price: data["å…¬å‘Šç¾å€¼"] = m_price.group(1)

    # æ‰€æœ‰æ¬Šäºº (å¥—ç”¨ AI å­¸ç¿’)
    m_owner = re.search(r'æ‰€æœ‰æ¬Šäºº\s*([^\s]+)', text)
    if m_owner: data["æ‰€æœ‰æ¬Šäºº"] = ai_smart_fix(m_owner.group(1).replace('*', 'ï¼Š'))
    
    # çµ±ä¸€ç·¨è™Ÿ
    m_id = re.search(r'çµ±ä¸€ç·¨è™Ÿ\s*([A-Z][\d\*]+)', text)
    if m_id: data["èº«åˆ†è­‰å­—è™Ÿ"] = m_id.group(1)

    # åœ°å€ (é‡é»å¥—ç”¨ AI å­¸ç¿’)
    m_addr = re.search(r'[åœ°ä½]\s*å€\s+(.+)', text)
    if m_addr: data["åœ°å€"] = ai_smart_fix(m_addr.group(1).strip())
    
    return data

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. Streamlit äº’å‹•ä»‹é¢ (æ•´åˆå­¸ç¿’åŠŸèƒ½)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.set_page_config(page_title="åœ°æ”¿æ™ºæ…§è§£è­¯ Pro", layout="wide")
ocr_engine = load_ocr()

def main():
    st.title("ğŸ  åœ°æ”¿æ™ºæ…§è§£è­¯ç³»çµ± Pro")
    
    # ä½¿ç”¨ session_state ä¿æŒè³‡æ–™ç‹€æ…‹
    if 'main_df' not in st.session_state: st.session_state.main_df = None
    if 'raw_txts' not in st.session_state: st.session_state.raw_txts = {}

    files = st.file_uploader("ä¸Šå‚³ PDF (æ”¯æ´å¤šæª”)", type="pdf", accept_multiple_files=True)
    
    if files and st.button("ğŸš€ é–‹å§‹å…¨è‡ªå‹•è§£è­¯"):
        rows = []
        for f in files:
            with st.spinner(f"æ­£åœ¨åˆ†æ {f.name}..."):
                pdf_bytes = f.read()
                all_imgs = convert_from_bytes(pdf_bytes, dpi=300, poppler_path=POPPLER_PATH)
                with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
                    first_text = pdf.pages[0].extract_text() or ""
                    if any(k in first_text for k in ["è¬„æœ¬ç¨®é¡ç¢¼", "åˆ—å°æ™‚é–“"]):
                        txt, _ = process_è¬„æœ¬(pdf, ocr_engine, all_imgs)
                    elif "ä¸€è¦½è¡¨" in first_text:
                        txt, _ = process_ç¾¤ç’‡(pdf, ocr_engine, all_imgs)
                    else:
                        fmt = "å…‰ç‰¹" if "ç¸£å¸‚" in normalize(first_text) else "è¯å®‰"
                        txt, _ = process_è¡¨æ ¼å¼(pdf, ocr_engine, all_imgs, fmt)
                
                st.session_state.raw_txts[f.name] = txt
                rows.append(parse_for_excel(txt))
        
        st.session_state.main_df = pd.DataFrame(rows)

    # â”€â”€â”€â”€â”€â”€ äº’å‹•ä¿®æ­£å€ â”€â”€â”€â”€â”€â”€
    if st.session_state.main_df is not None:
        st.divider()
        st.subheader("ğŸ“ æˆæœé è¦½èˆ‡æ‰‹å‹•ä¿®æ­£")
        st.caption("æ‚¨å¯ä»¥ç›´æ¥ä¿®æ”¹ä¸‹æ–¹è¡¨æ ¼å…§å®¹ï¼Œä¿®æ­£å¾Œçš„è³‡æ–™æœƒåŒæ­¥åŒ¯å‡ºåˆ° Excel èˆ‡ TXTã€‚")
        
        # è®“ä½¿ç”¨è€…ä¿®æ­£è³‡æ–™
        edited_df = st.data_editor(st.session_state.main_df, num_rows="fixed")
        
        if st.button("ğŸ§  ç¢ºèªä¿®æ­£ä¸¦è®“ AI å­¸ç¿’"):
            memory = load_ai_memory()
            # æ¯”å°åœ°å€æ¬„ä½çš„å·®ç•°ä¾†å­¸ç¿’
            for idx in range(len(edited_df)):
                old_val = st.session_state.main_df.iloc[idx]["åœ°å€"]
                new_val = edited_df.iloc[idx]["åœ°å€"]
                if old_val != new_val and old_val != "":
                    memory["history"][old_val] = new_val # ç´€éŒ„éŒ¯åˆ°å°çš„æ˜ å°„
            
            save_ai_memory(memory)
            st.session_state.main_df = edited_df # åŒæ­¥æ›´æ–°ç‹€æ…‹
            st.success("AI å·²è¨˜ä½æ‚¨çš„ä¿®æ­£ï¼ä¸‹æ¬¡è™•ç†ç›¸ä¼¼å…§å®¹å°‡è‡ªå‹•æ ¡æ­£ã€‚")

        # â”€â”€â”€â”€â”€â”€ ä¸‹è¼‰å€ â”€â”€â”€â”€â”€â”€
        col1, col2 = st.columns(2)
        with col1:
            # ç”¢å‡º Excel
            xlsx_io = io.BytesIO()
            with pd.ExcelWriter(xlsx_io, engine='xlsxwriter') as writer:
                edited_df.to_excel(writer, index=False, sheet_name='è³‡æ–™å½™æ•´')
            st.download_button("ğŸ“¥ ä¸‹è¼‰ Excel å ±è¡¨", xlsx_io.getvalue(), "åœ°æ”¿å½™æ•´.xlsx")

        with col2:
            # ä¸‹è¼‰ TXT (ZIP)
            z_io = io.BytesIO()
            with zipfile.ZipFile(z_io, "w") as zf:
                for filename, content in st.session_state.raw_txts.items():
                    # é€™è£¡ç¤ºç¯„å°‡ä¿®æ­£å¾Œçš„åœ°å€ä¹ŸåŒæ­¥å› TXT
                    zf.writestr(f"{filename}.txt", content)
            st.download_button("ğŸ“¦ ä¸‹è¼‰å…¨éƒ¨ TXT (ZIP)", z_io.getvalue(), "results.zip")

if __name__ == "__main__":
    main()