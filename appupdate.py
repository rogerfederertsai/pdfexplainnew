import streamlit as st
import pdfplumber
import io
import re
import cv2
import numpy as np
from pdf2image import convert_from_bytes
import easyocr
import zipfile
import unicodedata
import json
import os
import difflib
import pandas as pd  # æ–°å¢ï¼šç”¨æ–¼è™•ç† Excel

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ç’°å¢ƒé©æ‡‰èˆ‡è³‡æºè¼‰å…¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

LOCAL_POPPLER_PATH = r"C:\Users\User\Desktop\pdf_explain new\poppler-25.12.0\Library\bin"
POPPLER_PATH = LOCAL_POPPLER_PATH if os.path.exists(LOCAL_POPPLER_PATH) else None
CORRECTIONS_FILE = "addr_corrections.json"

@st.cache_resource
def load_ocr():
    return easyocr.Reader(['ch_tra', 'en'], gpu=False)

def normalize(text):
    if not text: return ""
    return unicodedata.normalize("NFKC", re.sub(r'\s+', '', text))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. æ–°å¢ï¼šExcel æ¬„ä½è§£æå‡½æ•¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def parse_for_excel(text):
    """å°‡è§£è­¯å¾Œçš„ TXT æ–‡å­—æ‹†è§£ç‚º Excel æ¬„ä½"""
    data = {
        "è¡Œæ”¿å€": "", "æ®µå°æ®µ": "", "åœ°è™Ÿ": "", "é¢ç©": "",
        "å…¬å‘Šç¾å€¼": "", "æ‰€æœ‰æ¬Šäºº": "", "èº«åˆ†è­‰å­—è™Ÿ": "", "åœ°å€": ""
    }
    
    # æå–åœ°æ®µèˆ‡åœ°è™Ÿ (Regex é‡å°å°ç£åœ°æ”¿æ ¼å¼è¨­è¨ˆ)
    m_land = re.search(r'([^\s]+(?:ç¸£|å¸‚)[^\s]+(?:å€|é„‰|é®|å¸‚))([^\s]+æ®µ)\s*([\d-]+)', text)
    if m_land:
        data["è¡Œæ”¿å€"] = m_land.group(1)
        data["æ®µå°æ®µ"] = m_land.group(2)
        data["åœ°è™Ÿ"] = m_land.group(3)

    # æå–é¢ç©
    m_area = re.search(r'é¢ç©\s*([\d.]+)', text)
    if m_area: data["é¢ç©"] = m_area.group(1)

    # æå–å…¬å‘Šç¾å€¼
    m_price = re.search(r'å…¬å‘ŠåœŸåœ°ç¾å€¼.*?(\d+)\s*å…ƒ', text)
    if m_price: data["å…¬å‘Šç¾å€¼"] = m_price.group(1)

    # æå–æ‰€æœ‰æ¬Šäººèˆ‡çµ±ç·¨
    m_owner = re.search(r'æ‰€æœ‰æ¬Šäºº\s*([^\s]+)', text)
    if m_owner: data["æ‰€æœ‰æ¬Šäºº"] = m_owner.group(1).replace('*', 'ï¼Š')
    
    m_id = re.search(r'çµ±ä¸€ç·¨è™Ÿ\s*([A-Z][\d\*]+)', text)
    if m_id: data["èº«åˆ†è­‰å­—è™Ÿ"] = m_id.group(1)

    # æå–åœ°å€
    m_addr = re.search(r'[åœ°ä½]\s*å€\s+(.+)', text)
    if m_addr: data["åœ°å€"] = m_addr.group(1).strip()
    
    return data

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. åœ°å€é©—è­‰ã€æ ¡æ­£èˆ‡ OCR ç­–ç•¥ (ä¿ç•™åŸæœ¬é‚è¼¯)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

TAIWAN_CITIES = [
    'è‡ºåŒ—å¸‚', 'æ–°åŒ—å¸‚', 'æ¡ƒåœ’å¸‚', 'è‡ºä¸­å¸‚', 'è‡ºå—å¸‚', 'é«˜é›„å¸‚',
    'åŸºéš†å¸‚', 'æ–°ç«¹å¸‚', 'å˜‰ç¾©å¸‚', 'æ–°ç«¹ç¸£', 'è‹—æ —ç¸£', 'å½°åŒ–ç¸£', 
    'å—æŠ•ç¸£', 'é›²æ—ç¸£', 'å˜‰ç¾©ç¸£', 'å±æ±ç¸£', 'å®œè˜­ç¸£', 'èŠ±è“®ç¸£', 
    'è‡ºæ±ç¸£', 'æ¾æ¹–ç¸£', 'é‡‘é–€ç¸£', 'é€£æ±Ÿç¸£', 'å°åŒ—å¸‚', 'å°ä¸­å¸‚', 
    'å°å—å¸‚', 'å°æ±ç¸£', 'å°åŒ—ç¸£', 'æ¡ƒåœ’ç¸£', 'å°ä¸­ç¸£', 'å°å—ç¸£', 'é«˜é›„ç¸£',
]

_CITY_LEVEL = {c: ('å¸‚' if c.endswith('å¸‚') else 'ç¸£') for c in TAIWAN_CITIES}
_DISTRICT_FOR_CITY = ['å€']
_DISTRICT_FOR_COUNTY = ['å€', 'é„‰', 'é®', 'å¸‚']

def load_corrections() -> dict:
    if os.path.exists(CORRECTIONS_FILE):
        try:
            with open(CORRECTIONS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except: return {}
    return {}

def save_correction(wrong: str, right: str):
    corrections = load_corrections()
    corrections[wrong.strip()] = right.strip()
    with open(CORRECTIONS_FILE, 'w', encoding='utf-8') as f:
        json.dump(corrections, f, ensure_ascii=False, indent=2)

def fix_addr_post_process(text: str) -> str:
    if not text: return text
    for wrong, right in {'è€‹': 'è‡º', 'è€¸': 'è‡º', 'å­¿': 'å­¸', 'å­½': 'å­¸', 'å£†': 'å­¸', 'è¦ƒ': 'å—'}.items():
        text = text.replace(wrong, right)
    text = re.sub(r'(\d)\s+(\d)', r'\1\2', text)
    _ADDR_CJK = r'[é‡Œé„°è·¯æ®µå··å¼„è™Ÿè¡—å€å¸‚ç¸£é„‰é®æ‘]'
    text = re.sub(rf'({_ADDR_CJK})\s+(\d)', r'\1\2', text)
    text = re.sub(rf'(\d)\s+({_ADDR_CJK})', r'\1\2', text)
    return text

def preprocess_for_ocr(img_gray: np.ndarray) -> list:
    imgs = []
    fx, fy = 4, 4
    b1 = cv2.resize(img_gray, None, fx=fx, fy=fy, interpolation=cv2.INTER_LANCZOS4)
    imgs.append(cv2.copyMakeBorder(b1, 30, 30, 30, 30, cv2.BORDER_CONSTANT, value=255))
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8)).apply(img_gray)
    b2 = cv2.resize(clahe, None, fx=fx, fy=fy, interpolation=cv2.INTER_LANCZOS4)
    imgs.append(cv2.copyMakeBorder(b2, 30, 30, 30, 30, cv2.BORDER_CONSTANT, value=255))
    return imgs

def ocr_with_best_result(ocr, img_gray: np.ndarray) -> tuple:
    strategies = ['åŸå§‹', 'CLAHE']
    candidates = []
    for i, img in enumerate(preprocess_for_ocr(img_gray)):
        results = ocr.readtext(img, detail=1, paragraph=False)
        raw = "".join([res[1] for res in results if normalize(res[1]) not in ['åœ°å€', 'ä½å€']]).strip()
        processed = fix_addr_post_process(raw)
        candidates.append((processed, strategies[i]))
    return max(candidates, key=lambda x: len(x[0])) if candidates else ("", "ç„¡çµæœ")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. æ–‡ä»¶è§£æé‚è¼¯ (åŸæœ¬çš„ 300 è¡Œæ ¸å¿ƒ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def clean_watermark(text):
    lines = text.split("\n")
    cleaned = []
    watermark_chars = set("è‡ºå—å¸‚æ–°åŒ–åœ°æ”¿äº‹å‹™æ‰€")
    for line in lines:
        s = line.strip()
        if s == "H0" or (len(s) == 1 and s in watermark_chars): continue
        line = re.sub(r'\s+[è‡ºå—å¸‚æ–°åŒ–åœ°æ”¿äº‹å‹™æ‰€]{1,2}\s*$', '', line)
        line = re.sub(r'è‡º(ä¸€èˆ¬è¾²æ¥­å€|éƒ½å¸‚ç™¼å±•å€|è¾²æ¥­å€)', r'\1', line)
        cleaned.append(line)
    return "\n".join(cleaned)

def extract_addr_from_image_stream(page, ocr, debug_log: list):
    words = page.extract_words()
    target = next((w for w in words if w['text'] in ['åœ°å€', 'ä½å€']), None)
    if not target: return ""
    label = "ä½" if target['text'] == 'ä½å€' else "åœ°"
    addr_imgs = [img for img in page.images if abs(img['top'] - target['top']) < 5]
    if not addr_imgs: return ""
    try:
        raw = addr_imgs[0]['stream'].get_data()
        buf = np.frombuffer(raw, dtype=np.uint8)
        decoded = cv2.imdecode(buf, cv2.IMREAD_GRAYSCALE)
        val, strat = ocr_with_best_result(ocr, decoded)
        debug_log.append(f"âœ… StreamæˆåŠŸ({strat}): {val}")
        return f"{label} å€ {val}"
    except: return ""

def ocr_addr_fallback(img_np, page, ocr, debug_log: list):
    h, w = img_np.shape[:2]
    sy, sx = h/page.height, w/page.width
    words = page.extract_words()
    target = next((w for w in words if w['text'] in ['åœ°å€', 'ä½å€']), None)
    if not target: return "[å®šä½å¤±æ•—]"
    next_w = [w for w in words if w['top'] > target['bottom'] + 1]
    bottom = next_w[0]['top'] if next_w else target['bottom'] + 20
    crop = img_np[max(0, int((target['top']-2)*sy)):min(h, int((bottom+2)*sy)), int(175*sx):]
    gray = cv2.cvtColor(crop, cv2.COLOR_RGB2GRAY)
    val, strat = ocr_with_best_result(ocr, gray)
    debug_log.append(f"âš ï¸ å‚™æ´æˆåŠŸ({strat}): {val}")
    return f"{('ä½' if target['text']=='ä½å€' else 'åœ°')} å€ {val or '[ç„¡æ³•è¾¨è­˜]'}"

def process_è¡¨æ ¼å¼(pdf, ocr, all_imgs, fmt):
    output, debug = [], []
    for i, page in enumerate(pdf.pages):
        page_text = []
        tables = page.extract_tables()
        if tables:
            for table in tables:
                for row in table:
                    cells = [c.strip().replace("\n", "") if c else "" for c in row]
                    if not any(cells): continue
                    is_addr = normalize(cells[0]) in ["åœ°å€", "ä½å€"]
                    has_content = any(c.strip() for c in cells[1:])
                    if is_addr and not has_content:
                        line = extract_addr_from_image_stream(page, ocr, debug)
                        if not line:
                            line = ocr_addr_fallback(np.array(all_imgs[i]), page, ocr, debug)
                    else:
                        line = "  ".join(c for c in cells if c)
                    page_text.append(line)
        output.append(f"===== ç¬¬ {i+1} é  =====\n" + "\n".join(page_text))
    return "\n\n".join(output), debug

def process_ç¾¤ç’‡(pdf, ocr, all_imgs):
    output = []
    for i, page in enumerate(pdf.pages):
        lines = []
        tables = page.extract_tables()
        for table in tables:
            for row in table:
                cells = [c.strip().replace("\n", "") if c else "" for c in row]
                if not any(cells) or any(x in "".join(cells) for x in ["ä¸€è¦½è¡¨", "åˆ—å°"]): continue
                if len(cells) >= 2 and normalize(cells[0]) in ["åœ°å€", "ä½å€"]:
                    lines.append(f"åœ°  å€  {cells[1]}")
                else:
                    lines.append("  ".join(c for c in cells if c))
        output.append(f"===== ç¬¬ {i+1} é  =====\n" + "\n".join(lines))
    return "\n\n".join(output), []

def process_è¬„æœ¬(pdf, ocr, all_imgs):
    output, debug = [], []
    for i, page in enumerate(pdf.pages):
        raw = clean_watermark(page.extract_text() or "")
        lines = raw.split("\n")
        res_lines = []
        for j, line in enumerate(lines):
            res_lines.append(line)
            if "æ‰€æœ‰æ¬Šäºº" in line:
                nxt = lines[j+1].strip() if j+1 < len(lines) else ""
                if "å€" not in nxt and "çµ±ä¸€ç·¨è™Ÿ" not in nxt:
                    img_np = np.array(all_imgs[i])
                    h, w = img_np.shape[:2]
                    scale = h/page.height
                    words = page.extract_words()
                    y = next((wd["top"] for wd in words if "æ‰€æœ‰æ¬Šäºº" in wd["text"]), None)
                    if y:
                        crop = img_np[int((y+10)*scale):int((y+70)*scale), :int(w*0.85)]
                        val, strat = ocr_with_best_result(ocr, cv2.cvtColor(crop, cv2.COLOR_RGB2GRAY))
                        if val: res_lines.append(f" ä½  å€ï¼š{val.replace('ä½å€ï¼š','')}")
        output.append(f"===== ç¬¬ {i+1} é  =====\n" + "\n".join(res_lines))
    return "\n\n".join(output), debug

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. ä¸»å…¥å£èˆ‡ Streamlit UI (æ•´åˆç‰ˆ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.set_page_config(page_title="åœ°æ”¿æ–‡ä»¶é€è¦–å™¨ Pro", layout="wide")
ocr = load_ocr()

def main():
    st.title("ğŸ  åœ°æ”¿æ–‡ä»¶é€è¦–å™¨ Pro")
    
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        show_debug = st.checkbox("é¡¯ç¤ºé™¤éŒ¯è³‡è¨Š")
        if st.button("ğŸ§¹ æ¸…é™¤æš«å­˜"):
            st.cache_resource.clear()
            st.rerun()

    files = st.file_uploader("ä¸Šå‚³ PDF (æ”¯æ´å¤šæª”)", type="pdf", accept_multiple_files=True)
    
    if files and st.button("ğŸš€ é–‹å§‹è™•ç†"):
        all_results = {}
        excel_rows = [] # ç”¨ä¾†å­˜ Excel æ¬„ä½çš„æ¸…å–®

        for f in files[:5]: # é™åˆ¶ä¸€æ¬¡æœ€å¤š5å€‹æª”æ¡ˆ
            with st.spinner(f"æ­£åœ¨è™•ç† {f.name}..."):
                pdf_bytes = f.read()
                all_imgs = convert_from_bytes(pdf_bytes, dpi=300, poppler_path=POPPLER_PATH)
                
                with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
                    text = pdf.pages[0].extract_text() or ""
                    # åˆ¤æ–·æ ¼å¼ä¸¦è™•ç†
                    if any(k in text for k in ["è¬„æœ¬ç¨®é¡ç¢¼", "åˆ—å°æ™‚é–“"]):
                        txt, dbg = process_è¬„æœ¬(pdf, ocr, all_imgs)
                    elif "ä¸€è¦½è¡¨" in text:
                        txt, dbg = process_ç¾¤ç’‡(pdf, ocr, all_imgs)
                    else:
                        fmt = "å…‰ç‰¹" if "ç¸£å¸‚" in normalize(text) else "è¯å®‰"
                        txt, dbg = process_è¡¨æ ¼å¼(pdf, ocr, all_imgs, fmt)
                
                all_results[f.name] = txt
                
                # --- æ–°å¢ï¼šå°‡è§£è­¯æ–‡å­—è§£ææˆ Excel æ¬„ä½ ---
                excel_rows.append(parse_for_excel(txt))
                
                # åŸæœ‰çš„ TXT åŠŸèƒ½é¡¯ç¤º
                st.success(f"âœ… {f.name} å®Œæˆ")
                if show_debug:
                    with st.expander(f"ğŸ” {f.name} é™¤éŒ¯æ—¥èªŒ"):
                        for d in dbg: st.text(d)
                
                st.text_area(f"é è¦½: {f.name}", txt, height=150)
                st.download_button(f"ä¸‹è¼‰ {f.name}.txt", txt, f"{f.name}.txt", key=f"dl_{f.name}")

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 6. æ–°å¢ï¼šç”¢å‡º Excel å ±è¡¨
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if excel_rows:
            st.divider()
            st.header("ğŸ“Š å…¨æ¡ˆ Excel å ±è¡¨å½™æ•´")
            df = pd.DataFrame(excel_rows)
            
            # è¨­å®šæ¬„ä½é †åº
            cols = ["è¡Œæ”¿å€", "æ®µå°æ®µ", "åœ°è™Ÿ", "é¢ç©", "å…¬å‘Šç¾å€¼", "æ‰€æœ‰æ¬Šäºº", "èº«åˆ†è­‰å­—è™Ÿ", "åœ°å€"]
            df = df[cols]
            
            st.dataframe(df) # ç¶²é é¡¯ç¤ºé è¦½è¡¨æ ¼

            # ä¸‹è¼‰ Excel æŒ‰éˆ•
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                df.to_excel(writer, index=False, sheet_name='è³‡æ–™å½™æ•´')
            
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰å½™æ•´ Excel å ±è¡¨",
                data=output.getvalue(),
                file_name="åœ°æ”¿è§£è­¯çµæœå½™æ•´.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

        # ZIP ä¸‹è¼‰æ‰€æœ‰ TXT
        if len(all_results) > 1:
            z_buf = io.BytesIO()
            with zipfile.ZipFile(z_buf, "w") as zf:
                for n, c in all_results.items(): zf.writestr(f"{n}.txt", c)
            st.download_button("ğŸ“¦ ä¸‹è¼‰å…¨éƒ¨æ–‡å­—æª” (ZIP)", z_buf.getvalue(), "results.zip")

if __name__ == "__main__":
    main()