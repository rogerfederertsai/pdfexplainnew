import streamlit as st
import pdfplumber
import io
import re
import cv2
import numpy as np
from pdf2image import convert_from_bytes
import easyocr
import zipfile
import unicodedata
import pandas as pd
import gspread
import os
import difflib
from google.oauth2.service_account import Credentials

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. é›²ç«¯è¨˜æ†¶æ¨¡çµ„ (ç¢ºä¿ä¸å¿«å–ï¼Œæ¯æ¬¡éƒ½æŠ“æœ€æ–°)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_gsheet_client():
    scopes = ["https://www.googleapis.com/auth/spreadsheets", "https://www.googleapis.com/auth/drive"]
    try:
        creds_info = st.secrets["gcp_service_account"]
        creds = Credentials.from_service_account_info(creds_info, scopes=scopes)
        gc = gspread.authorize(creds)
        return gc.open("åœ°æ”¿AIå­¸ç¿’åº«").sheet1
    except Exception as e:
        return None

def load_cloud_memory():
    """å¼·åˆ¶æŠ“å–é›²ç«¯æœ€æ–°è³‡æ–™ï¼Œä¸ä½¿ç”¨ cache"""
    sheet = get_gsheet_client()
    if sheet:
        try:
            records = sheet.get_all_records()
            return {str(r['wrong']): str(r['right']) for r in records if 'wrong' in r}
        except:
            return {}
    return {}

def save_to_cloud(wrong, right):
    sheet = get_gsheet_client()
    if sheet:
        try:
            sheet.append_row([str(wrong), str(right)])
        except:
            pass

def ai_smart_fix(text, memory):
    """æ‡‰ç”¨å‚³å…¥çš„æœ€æ–° memory é€²è¡Œæ›¿æ›"""
    if not text or not memory: return text
    # å„ªå…ˆæ›¿æ›é•·çš„é—œéµå­—ï¼ˆç’°å¢ƒé—œéµå­—ï¼‰ï¼Œç¢ºä¿ç²¾æº–åº¦
    sorted_keys = sorted(memory.keys(), key=len, reverse=True)
    for wrong_key in sorted_keys:
        if wrong_key in text:
            text = text.replace(wrong_key, str(memory[wrong_key]))
    return text

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. åŸæœ‰æˆåŠŸè¾¨è­˜é‚è¼¯ (å®Œå…¨ä¿ç•™ä¸å‹•)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LOCAL_POPPLER_PATH = r"C:\Users\User\Desktop\pdf_explain new\poppler-25.12.0\Library\bin"
POPPLER_PATH = LOCAL_POPPLER_PATH if os.path.exists(LOCAL_POPPLER_PATH) else None

@st.cache_resource
def load_ocr():
    return easyocr.Reader(['ch_tra', 'en'], gpu=False)

def normalize(text):
    if not text: return ""
    return unicodedata.normalize("NFKC", re.sub(r'\s+', '', text))

# [ä¿ç•™æ‚¨åŸæœ‰çš„ fix_addr_post_process, ocr_with_best_result, extract_addr_from_image_stream]
def fix_addr_post_process(text: str) -> str:
    if not text: return text
    _ADDR_CHAR_MAP = {'è€‹': 'è‡º', 'è€¸': 'è‡º', 'å­¿': 'å­¸', 'å­½': 'å­¸', 'å£†': 'å­¸', 'è¦ƒ': 'å—'}
    for wrong, right in _ADDR_CHAR_MAP.items():
        text = text.replace(wrong, right)
    text = re.sub(r'(\d)\s+(\d)', r'\1\2', text)
    _ADDR_CJK = r'[é‡Œé„°è·¯æ®µå··å¼„è™Ÿè¡—å€å¸‚ç¸£é„‰é®æ‘]'
    text = re.sub(rf'({_ADDR_CJK})\s+(\d)', r'\1\2', text)
    text = re.sub(rf'(\d)\s+({_ADDR_CJK})', r'\1\2', text)
    return text

# [ä¿ç•™åŸæœ‰çš„ä¸‰å¤§è§£æè™•ç†å™¨ process_è¡¨æ ¼å¼, process_ç¾¤ç’‡, process_è¬„æœ¬]
def process_è¡¨æ ¼å¼(pdf, ocr, all_imgs, fmt):
    output, debug = [], []
    for i, page in enumerate(pdf.pages):
        page_text = []
        tables = page.extract_tables()
        for table in tables:
            for row in table:
                cells = [c.strip().replace("\n", "") if c else "" for c in row]
                if not any(cells): continue
                if normalize(cells[0]) in ["åœ°å€", "ä½å€"] and not any(cells[1:]):
                    line = extract_addr_from_image_stream(page, ocr, debug)
                else:
                    line = "  ".join(c for c in cells if c)
                page_text.append(line)
        output.append(f"===== ç¬¬ {i+1} é  =====\n" + "\n".join(page_text))
    return "\n\n".join(output), debug

def process_ç¾¤ç’‡(pdf, ocr, all_imgs):
    output = []
    for i, page in enumerate(pdf.pages):
        lines = [ "  ".join(filter(None, [c.replace("\n","") for c in row])) for table in page.extract_tables() for row in table ]
        output.append(f"===== ç¬¬ {i+1} é  =====\n" + "\n".join(lines))
    return "\n\n".join(output), []

def process_è¬„æœ¬(pdf, ocr, all_imgs):
    output = []
    for i, page in enumerate(pdf.pages):
        txt = page.extract_text() or ""
        output.append(f"===== ç¬¬ {i+1} é  =====\n" + txt)
    return "\n\n".join(output), []

def ocr_with_best_result(ocr, img_gray: np.ndarray) -> tuple:
    fx, fy = 4, 4
    b1 = cv2.resize(img_gray, None, fx=fx, fy=fy, interpolation=cv2.INTER_LANCZOS4)
    results = ocr.readtext(b1, detail=0)
    raw = "".join(results).strip()
    processed = fix_addr_post_process(raw)
    return processed, "Standard"

def extract_addr_from_image_stream(page, ocr, debug_log: list):
    words = page.extract_words()
    target = next((w for w in words if w['text'] in ['åœ°å€', 'ä½å€']), None)
    if not target: return ""
    addr_imgs = [img for img in page.images if abs(img['top'] - target['top']) < 5]
    if not addr_imgs: return ""
    try:
        raw = addr_imgs[0]['stream'].get_data()
        buf = np.frombuffer(raw, dtype=np.uint8)
        decoded = cv2.imdecode(buf, cv2.IMREAD_GRAYSCALE)
        val, _ = ocr_with_best_result(ocr, decoded)
        return f"{target['text']} {val}"
    except: return ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. Excel è§£æ (åœ¨æ­¤è™•ç«‹å³å¥—ç”¨æœ€æ–°è¨˜æ†¶)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_for_excel(text, memory):
    # é‡é»ï¼šåœ¨è§£ææ¬„ä½å‰ï¼Œå…ˆæ‹¿æœ€æ–°è¨˜æ†¶åˆ·æ´—ä¸€æ¬¡æ–‡å­—
    text = ai_smart_fix(text, memory)
    
    data = {
        "è¡Œæ”¿å€/æ®µ": "", "åœ°è™Ÿ": "", "é¢ç©(m2)": "", 
        "å…¬å‘ŠåœŸåœ°ç¾å€¼": "", "æ‰€æœ‰æ¬Šäºº": "", "çµ±ä¸€ç·¨è™Ÿ": "", "åœ°å€": ""
    }
    m_loc = re.search(r'([^\s]+(?:ç¸£|å¸‚)[^\s]+(?:å€|é„‰|é®|å¸‚)[^\s]+æ®µ)', text)
    if m_loc: data["è¡Œæ”¿å€/æ®µ"] = m_loc.group(1)
    
    m_no = re.search(r'(\d{4}-\d{4})', text)
    if m_no: data["åœ°è™Ÿ"] = m_no.group(1)
    
    m_area = re.search(r'é¢ç©\s*[,ï¼Œ]?\s*([\d.]+)', text)
    if m_area: data["é¢ç©(m2)"] = m_area.group(1)
    
    m_price = re.search(r'å…¬å‘ŠåœŸåœ°ç¾å€¼.*?(\d+)\s*å…ƒ', text)
    if m_price: data["å…¬å‘ŠåœŸåœ°ç¾å€¼"] = m_price.group(1)
    
    m_owner = re.search(r'æ‰€æœ‰æ¬Šäºº\s*[,ï¼Œ]?\s*([^\s,ï¼Œ]+)', text)
    if m_owner: data["æ‰€æœ‰æ¬Šäºº"] = m_owner.group(1).replace('*', 'ï¼Š')
    
    m_id = re.search(r'çµ±ä¸€ç·¨è™Ÿ\s*[,ï¼Œ]?\s*([A-Z\d\*]+)', text)
    if m_id: data["çµ±ä¸€ç·¨è™Ÿ"] = m_id.group(1)

    m_addr = re.search(r'åœ°\s*å€\s*[,ï¼Œ]?\s*(.+)', text)
    if m_addr: data["åœ°å€"] = m_addr.group(1).strip()
    
    return data

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. Streamlit ä¸»æµç¨‹
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="åœ°æ”¿è§£è­¯ç©©å®šç‰ˆ", layout="wide")
ocr_engine = load_ocr()

def main():
    st.title("ğŸ  åœ°æ”¿ AI æ™ºæ…§è§£è­¯ (åŒæ­¥åˆ·æ–°ç‰ˆ)")
    
    if 'main_df' not in st.session_state: st.session_state.main_df = None
    if 'raw_txts' not in st.session_state: st.session_state.raw_txts = {}

    files = st.file_uploader("ä¸Šå‚³ PDF", type="pdf", accept_multiple_files=True)
    
    if files and st.button("ğŸš€ é–‹å§‹è§£è­¯"):
        # å¼·åˆ¶åœ¨æ­¤åˆ»é‡æ–°è®€å–é›²ç«¯ï¼Œä¿è­‰ã€Œç§’è¨˜ã€
        latest_mem = load_cloud_memory()
        rows = []
        for f in files:
            with st.spinner(f"åˆ†æä¸­: {f.name}"):
                pdf_bytes = f.read()
                all_imgs = convert_from_bytes(pdf_bytes, dpi=300, poppler_path=POPPLER_PATH)
                with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
                    first_text = pdf.pages[0].extract_text() or ""
                    if any(k in first_text for k in ["è¬„æœ¬ç¨®é¡ç¢¼", "åˆ—å°æ™‚é–“"]):
                        txt, _ = process_è¬„æœ¬(pdf, ocr_engine, all_imgs)
                    elif "ä¸€è¦½è¡¨" in first_text:
                        txt, _ = process_ç¾¤ç’‡(pdf, ocr_engine, all_imgs)
                    else:
                        fmt = "å…‰ç‰¹" if "ç¸£å¸‚" in normalize(first_text) else "è¯å®‰"
                        txt, _ = process_è¡¨æ ¼å¼(pdf, ocr_engine, all_imgs, fmt)
                
                # å­˜å…¥æ™‚ä¹ŸåŒæ­¥ä¿®æ­£ raw_txts
                fixed_txt = ai_smart_fix(txt, latest_mem)
                st.session_state.raw_txts[f.name] = fixed_txt
                rows.append(parse_for_excel(txt, latest_mem))
        st.session_state.main_df = pd.DataFrame(rows)

    if st.session_state.main_df is not None:
        st.subheader("ğŸ“ æˆæœä¿®æ­£èˆ‡ AI å­¸ç¿’")
        edited_df = st.data_editor(st.session_state.main_df, num_rows="fixed")
        
        if st.button("ğŸ§  å„²å­˜ä¿®æ­£ (å‹•æ…‹çª—å£å­¸ç¿’)"):
            for idx in range(len(edited_df)):
                for col in ["åœ°å€", "æ‰€æœ‰æ¬Šäºº"]:
                    old_v = str(st.session_state.main_df.iloc[idx][col])
                    new_v = str(edited_df.iloc[idx][col])
                    if old_v != new_v and old_v != "":
                        # å‹•æ…‹çª—å£æ‰¾å‡ºé€£çºŒéŒ¯èª¤å€å¡Š
                        diff = list(difflib.ndiff(old_v, new_v))
                        i = 0
                        while i < len(diff):
                            if diff[i].startswith('- '):
                                wrong_block, right_block = "", ""
                                start_idx = i
                                while i < len(diff) and (diff[i].startswith('- ') or diff[i].startswith('+ ')):
                                    if diff[i].startswith('- '): wrong_block += diff[i][2:]
                                    if diff[i].startswith('+ '): right_block += diff[i][2:]
                                    i += 1
                                prefix = diff[start_idx-1][2:] if start_idx > 0 and diff[start_idx-1].startswith('  ') else ""
                                suffix = diff[i][2:] if i < len(diff) and diff[i].startswith('  ') else ""
                                save_to_cloud(f"{prefix}{wrong_block}{suffix}", f"{prefix}{right_block}{suffix}")
                            else: i += 1
            st.session_state.main_df = edited_df
            st.success("AI å­¸ç¿’å®Œæˆï¼ä¸‹æ¬¡è§£è­¯å°‡è‡ªå‹•ä¿®æ­£ã€‚")

        # â”€â”€â”€â”€â”€â”€ ä¸‹è¼‰å€ (ä¸‹è¼‰æ™‚å†æ¬¡å¼·åˆ¶ç¢ºèª) â”€â”€â”€â”€â”€â”€
        c1, c2 = st.columns(2)
        with c1:
            xlsx_io = io.BytesIO()
            edited_df.to_excel(xlsx_io, index=False)
            st.download_button("ğŸ“¥ ä¸‹è¼‰ Excel", xlsx_io.getvalue(), "åœ°æ”¿å½™æ•´.xlsx")
        
        with c2:
            z_io = io.BytesIO()
            # ä¸‹è¼‰å‰å†åˆ·ä¸€æ¬¡æœ€æ–°çš„é›²ç«¯
            final_mem = load_cloud_memory() 
            with zipfile.ZipFile(z_io, "w") as zf:
                for fname, content in st.session_state.raw_txts.items():
                    zf.writestr(f"{fname}.txt", ai_smart_fix(content, final_mem))
            st.download_button("ğŸ“¦ ä¸‹è¼‰ä¿®æ­£å¾Œ TXT (ZIP)", z_io.getvalue(), "results.zip")

if __name__ == "__main__":
    main()