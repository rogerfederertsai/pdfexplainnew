import streamlit as st
import pdfplumber
import io
import re
import cv2
import numpy as np
from pdf2image import convert_from_bytes
import easyocr
import zipfile
import unicodedata
import json
import os
import pandas as pd
import gspread
from google.oauth2.service_account import Credentials

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. Google Sheets é›²ç«¯é€£ç·šæ¨¡çµ„ (å–ä»£åŸæœ¬çš„ JSON æª”)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_gsheet_client():
    """é€é Streamlit Secrets é€£çµ Google Sheets"""
    scopes = ["https://www.googleapis.com/auth/spreadsheets", "https://www.googleapis.com/auth/drive"]
    try:
        # å¾ Secrets è®€å–æ‚¨å¡«å¯«çš„ TOML é‡‘é‘°
        creds_info = st.secrets["gcp_service_account"]
        creds = Credentials.from_service_account_info(creds_info, scopes=scopes)
        gc = gspread.authorize(creds)
        # é–‹å•Ÿè©¦ç®—è¡¨ (è«‹ç¢ºä¿è©¦ç®—è¡¨åç¨±æ­£ç¢º)
        return gc.open("åœ°æ”¿AIå­¸ç¿’åº«").sheet1
    except Exception as e:
        return None

def load_cloud_memory():
    """å¾é›²ç«¯è®€å–æ‰€æœ‰å­¸ç¿’éçš„ä¿®æ­£ç´€éŒ„"""
    sheet = get_gsheet_client()
    if sheet:
        try:
            records = sheet.get_all_records()
            # å»ºç«‹å°ç…§å­—å…¸ï¼Œä¾‹å¦‚ {"å…¬å­½è·¯": "å…¬å­¸è·¯"}
            return {str(r['wrong']): str(r['right']) for r in records if 'wrong' in r}
        except: return {}
    return {}

def save_to_cloud(wrong, right):
    """å°‡ä¿®æ­£çµæœæ°¸ä¹…å­˜å…¥ Google Sheets"""
    sheet = get_gsheet_client()
    if sheet:
        try:
            sheet.append_row([str(wrong), str(right)])
        except: pass

def ai_smart_fix(text):
    """è‡ªå‹•å¥—ç”¨ AI å­¸ç¿’éçš„ä¿®æ­£è¡Œç‚º (å¤–æ›å¼ä¸å½±éŸ¿åŸé‚è¼¯)"""
    if not text: return text
    memory = load_cloud_memory()
    for wrong, right in memory.items():
        if str(wrong) in text:
            text = text.replace(str(wrong), str(right))
    return text

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ç’°å¢ƒé©æ‡‰èˆ‡è³‡æºè¼‰å…¥ (ä¿ç•™åŸé‚è¼¯)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

LOCAL_POPPLER_PATH = r"C:\Users\User\Desktop\pdf_explain new\poppler-25.12.0\Library\bin"
POPPLER_PATH = LOCAL_POPPLER_PATH if os.path.exists(LOCAL_POPPLER_PATH) else None

@st.cache_resource
def load_ocr():
    return easyocr.Reader(['ch_tra', 'en'], gpu=False)

def normalize(text):
    if not text: return ""
    return unicodedata.normalize("NFKC", re.sub(r'\s+', '', text))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. æ ¸å¿ƒ OCR ç­–ç•¥èˆ‡åœ°å€æ ¡æ­£ (æ‚¨çš„åŸé‚è¼¯)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fix_addr_post_process(text: str) -> str:
    if not text: return text
    # å…ˆå¥—ç”¨ AI é›²ç«¯å­¸ç¿’çµæœ
    text = ai_smart_fix(text)
    # åŸ·è¡Œæ‚¨åŸæœ¬æˆåŠŸçš„ç¡¬ç·¨ç¢¼æ ¡æ­£
    _ADDR_CHAR_MAP = {'è€‹': 'è‡º', 'è€¸': 'è‡º', 'å­¿': 'å­¸', 'å­½': 'å­¸', 'å£†': 'å­¸', 'è¦ƒ': 'å—'}
    for wrong, right in _ADDR_CHAR_MAP.items():
        text = text.replace(wrong, right)
    text = re.sub(r'(\d)\s+(\d)', r'\1\2', text)
    _ADDR_CJK = r'[é‡Œé„°è·¯æ®µå··å¼„è™Ÿè¡—å€å¸‚ç¸£é„‰é®æ‘]'
    text = re.sub(rf'({_ADDR_CJK})\s+(\d)', r'\1\2', text)
    text = re.sub(rf'(\d)\s+({_ADDR_CJK})', r'\1\2', text)
    return text

def ocr_with_best_result(ocr, img_gray: np.ndarray) -> tuple:
    fx, fy = 4, 4
    b1 = cv2.resize(img_gray, None, fx=fx, fy=fy, interpolation=cv2.INTER_LANCZOS4)
    results = ocr.readtext(b1, detail=0)
    raw = "".join(results).strip()
    processed = fix_addr_post_process(raw)
    return processed, "Standard"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. æ–‡ä»¶è§£æé‚è¼¯ (æ‚¨çš„åŸé‚è¼¯ï¼šè¬„æœ¬ã€ç¾¤ç’‡ã€è¡¨æ ¼å¼)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def extract_addr_from_image_stream(page, ocr, debug_log: list):
    words = page.extract_words()
    target = next((w for w in words if w['text'] in ['åœ°å€', 'ä½å€']), None)
    if not target: return ""
    addr_imgs = [img for img in page.images if abs(img['top'] - target['top']) < 5]
    if not addr_imgs: return ""
    try:
        raw = addr_imgs[0]['stream'].get_data()
        buf = np.frombuffer(raw, dtype=np.uint8)
        decoded = cv2.imdecode(buf, cv2.IMREAD_GRAYSCALE)
        val, _ = ocr_with_best_result(ocr, decoded)
        return f"{target['text']} {val}"
    except: return ""

def process_è¡¨æ ¼å¼(pdf, ocr, all_imgs, fmt):
    output, debug = [], []
    for i, page in enumerate(pdf.pages):
        page_text = []
        tables = page.extract_tables()
        for table in tables:
            for row in table:
                cells = [c.strip().replace("\n", "") if c else "" for c in row]
                if not any(cells): continue
                if normalize(cells[0]) in ["åœ°å€", "ä½å€"] and not any(cells[1:]):
                    line = extract_addr_from_image_stream(page, ocr, debug)
                else:
                    line = "  ".join(c for c in cells if c)
                page_text.append(line)
        output.append(f"===== ç¬¬ {i+1} é  =====\n" + "\n".join(page_text))
    return "\n\n".join(output), debug

def process_ç¾¤ç’‡(pdf, ocr, all_imgs):
    output = []
    for i, page in enumerate(pdf.pages):
        lines = [ "  ".join(filter(None, [c.replace("\n","") for c in row])) for table in page.extract_tables() for row in table ]
        output.append(f"===== ç¬¬ {i+1} é  =====\n" + "\n".join(lines))
    return "\n\n".join(output), []

def process_è¬„æœ¬(pdf, ocr, all_imgs):
    output = []
    for i, page in enumerate(pdf.pages):
        txt = page.extract_text() or ""
        output.append(f"===== ç¬¬ {i+1} é  =====\n" + txt)
    return "\n\n".join(output), []

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. Excel çµæ§‹åŒ–è§£æ (ä¸²æ¥ AI é›²ç«¯å­¸ç¿’)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def parse_for_excel(text):
    # è§£æå‰å…ˆå¥—ç”¨é›²ç«¯è¨˜æ†¶
    text = ai_smart_fix(text)
    
    data = {"è¡Œæ”¿å€": "", "æ®µå°æ®µ": "", "åœ°è™Ÿ": "", "é¢ç©": "", "å…¬å‘Šç¾å€¼": "", "æ‰€æœ‰æ¬Šäºº": "", "èº«åˆ†è­‰å­—è™Ÿ": "", "åœ°å€": ""}
    m_land = re.search(r'([^\s]+(?:ç¸£|å¸‚)[^\s]+(?:å€|é„‰|é®|å¸‚))([^\s]+æ®µ)\s*([\d-]+)', text)
    if m_land: data["è¡Œæ”¿å€"], data["æ®µå°æ®µ"], data["åœ°è™Ÿ"] = m_land.groups()

    m_area = re.search(r'é¢ç©\s*([\d.]+)', text)
    if m_area: data["é¢ç©"] = m_area.group(1)

    m_owner = re.search(r'æ‰€æœ‰æ¬Šäºº\s*([^\s]+)', text)
    if m_owner: 
        owner_name = m_owner.group(1).replace('*', 'ï¼Š')
        data["æ‰€æœ‰æ¬Šäºº"] = ai_smart_fix(owner_name)
    
    m_addr = re.search(r'[åœ°ä½]\s*å€\s+(.+)', text)
    if m_addr: data["åœ°å€"] = ai_smart_fix(m_addr.group(1).strip())
    
    return data

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. Streamlit ä»‹é¢ (é›²ç«¯è¨˜æ†¶ç©©å®šç‰ˆ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.set_page_config(page_title="åœ°æ”¿æ™ºæ…§è§£è­¯é›²ç«¯ç‰ˆ", layout="wide")
ocr_engine = load_ocr()

def main():
    st.title("ğŸ  åœ°æ”¿æ™ºæ…§è§£è­¯ç³»çµ± (é›²ç«¯æ°¸ä¹…å­¸ç¿’ç‰ˆ)")
    
    if 'main_df' not in st.session_state: st.session_state.main_df = None
    if 'raw_txts' not in st.session_state: st.session_state.raw_txts = {}

    with st.sidebar:
        st.header("âš™ï¸ é›²ç«¯ç‹€æ…‹")
        if get_gsheet_client():
            st.success("âœ… é›²ç«¯è¨˜æ†¶åº«å·²é€£ç·š")
        else:
            st.error("âŒ é›²ç«¯é€£ç·šå¤±æ•— (è«‹æª¢æŸ¥ Secrets)")

    files = st.file_uploader("ä¸Šå‚³ PDF (æ”¯æ´å¤šæª”)", type="pdf", accept_multiple_files=True)
    
    if files and st.button("ğŸš€ é–‹å§‹å…¨è‡ªå‹•è§£è­¯"):
        rows = []
        for f in files:
            with st.spinner(f"æ­£åœ¨åˆ†æ {f.name}..."):
                pdf_bytes = f.read()
                all_imgs = convert_from_bytes(pdf_bytes, dpi=300, poppler_path=POPPLER_PATH)
                with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
                    first_text = pdf.pages[0].extract_text() or ""
                    if any(k in first_text for k in ["è¬„æœ¬ç¨®é¡ç¢¼", "åˆ—å°æ™‚é–“"]):
                        txt, _ = process_è¬„æœ¬(pdf, ocr_engine, all_imgs)
                    elif "ä¸€è¦½è¡¨" in first_text:
                        txt, _ = process_ç¾¤ç’‡(pdf, ocr_engine, all_imgs)
                    else:
                        fmt = "å…‰ç‰¹" if "ç¸£å¸‚" in normalize(first_text) else "è¯å®‰"
                        txt, _ = process_è¡¨æ ¼å¼(pdf, ocr_engine, all_imgs, fmt)
                
                st.session_state.raw_txts[f.name] = txt
                rows.append(parse_for_excel(txt))
        
        st.session_state.main_df = pd.DataFrame(rows)

    # â”€â”€â”€â”€â”€â”€ äº’å‹•ä¿®æ­£å€ â”€â”€â”€â”€â”€â”€
    if st.session_state.main_df is not None:
        st.divider()
        st.subheader("ğŸ“ æˆæœé è¦½èˆ‡æ‰‹å‹•ä¿®æ­£")
        
        # ä½¿ç”¨ä½¿ç”¨è€…ç›´æ¥ä¿®æ”¹çš„çµæœ
        edited_df = st.data_editor(st.session_state.main_df, num_rows="fixed")
        
        if st.button("ğŸ§  ç¢ºèªä¿®æ­£ä¸¦è¨“ç·´ AI (æ°¸ä¹…å„²å­˜)"):
            # æ‰¾å‡ºã€Œåœ°å€ã€æˆ–ã€Œæ‰€æœ‰æ¬Šäººã€çš„è®Šå‹•
            for idx in range(len(edited_df)):
                for col in ["åœ°å€", "æ‰€æœ‰æ¬Šäºº"]:
                    old_v = str(st.session_state.main_df.iloc[idx][col])
                    new_v = str(edited_df.iloc[idx][col])
                    if old_v != new_v and old_v != "":
                        save_to_cloud(old_v, new_v) # å­˜å…¥ Google Sheets
            
            st.session_state.main_df = edited_df
            st.success("ğŸ‰ AI å­¸ç¿’å®Œæˆï¼ä¿®æ­£çµæœå·²å­˜å…¥é›²ç«¯ã€‚")
            st.rerun()

        # â”€â”€â”€â”€â”€â”€ ä¸‹è¼‰å€ â”€â”€â”€â”€â”€â”€
        col1, col2 = st.columns(2)
        with col1:
            xlsx_io = io.BytesIO()
            with pd.ExcelWriter(xlsx_io, engine='xlsxwriter') as writer:
                edited_df.to_excel(writer, index=False, sheet_name='è³‡æ–™å½™æ•´')
            st.download_button("ğŸ“¥ ä¸‹è¼‰ Excel å ±è¡¨", xlsx_io.getvalue(), "åœ°æ”¿å½™æ•´.xlsx")

        with col2:
            z_io = io.BytesIO()
            with zipfile.ZipFile(z_io, "w") as zf:
                for filename, content in st.session_state.raw_txts.items():
                    # TXT ä¹Ÿå¥—ç”¨ AI ä¿®æ­£å¾Œç”¢å‡º
                    zf.writestr(f"{filename}.txt", ai_smart_fix(content))
            st.download_button("ğŸ“¦ ä¸‹è¼‰å…¨éƒ¨ TXT (ZIP)", z_io.getvalue(), "results.zip")

if __name__ == "__main__":
    main()