import streamlit as st
import pdfplumber
import io
import re
import cv2
import numpy as np
from pdf2image import convert_from_bytes
import easyocr
import zipfile
import unicodedata
import json
import os
import difflib

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  ç’°å¢ƒè¨­å®šï¼šè‡ªå‹•åµæ¸¬ç’°å¢ƒ (Local vs Cloud)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å¦‚æœåœ¨æœ¬æ©Ÿæ¸¬è©¦ï¼Œè«‹å°‡ Poppler æ”¾åœ¨å°ˆæ¡ˆè³‡æ–™å¤¾å…§æˆ–è¨­å®šç’°å¢ƒè®Šæ•¸
LOCAL_POPPLER_PATH = r"C:\Users\User\Desktop\pdf_explain new\poppler-25.12.0\Library\bin"
POPPLER_PATH = LOCAL_POPPLER_PATH if os.path.exists(LOCAL_POPPLER_PATH) else None
CORRECTIONS_FILE = "addr_corrections.json"

@st.cache_resource
def load_ocr():
    # åœ¨é›²ç«¯ç’°å¢ƒ gpu=False æ˜¯å¿…é ˆçš„ï¼Œé™¤éä½ æœ‰ä»˜è²»å‡ç´š GPU è³‡æº
    return easyocr.Reader(['ch_tra', 'en'], gpu=False)

def normalize(text):
    if not text: return ""
    return unicodedata.normalize("NFKC", re.sub(r'\s+', '', text))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  å°ç£ç¸£å¸‚æ¸…å–®èˆ‡æ ¡æ­£é‚è¼¯
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TAIWAN_CITIES = [
    'è‡ºåŒ—å¸‚', 'æ–°åŒ—å¸‚', 'æ¡ƒåœ’å¸‚', 'è‡ºä¸­å¸‚', 'è‡ºå—å¸‚', 'é«˜é›„å¸‚',
    'åŸºéš†å¸‚', 'æ–°ç«¹å¸‚', 'å˜‰ç¾©å¸‚', 'æ–°ç«¹ç¸£', 'è‹—æ —ç¸£', 'å½°åŒ–ç¸£', 
    'å—æŠ•ç¸£', 'é›²æ—ç¸£', 'å˜‰ç¾©ç¸£', 'å±æ±ç¸£', 'å®œè˜­ç¸£', 'èŠ±è“®ç¸£', 
    'è‡ºæ±ç¸£', 'æ¾æ¹–ç¸£', 'é‡‘é–€ç¸£', 'é€£æ±Ÿç¸£', 'å°åŒ—å¸‚', 'å°ä¸­å¸‚', 
    'å°å—å¸‚', 'å°æ±ç¸£', 'å°åŒ—ç¸£', 'æ¡ƒåœ’ç¸£', 'å°ä¸­ç¸£', 'å°å—ç¸£', 'é«˜é›„ç¸£',
]

_CITY_LEVEL = {c: ('å¸‚' if c.endswith('å¸‚') else 'ç¸£') for c in TAIWAN_CITIES}
_DISTRICT_FOR_CITY = ['å€']
_DISTRICT_FOR_COUNTY = ['å€', 'é„‰', 'é®', 'å¸‚']

def load_corrections() -> dict:
    if os.path.exists(CORRECTIONS_FILE):
        try:
            with open(CORRECTIONS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return {}
    return {}

def save_correction(wrong: str, right: str):
    corrections = load_corrections()
    corrections[wrong.strip()] = right.strip()
    with open(CORRECTIONS_FILE, 'w', encoding='utf-8') as f:
        json.dump(corrections, f, ensure_ascii=False, indent=2)

def apply_corrections(text: str) -> str:
    corrections = load_corrections()
    return corrections.get(text.strip(), text)

def validate_addr_prefix(text: str) -> bool:
    return any(text.startswith(city) for city in TAIWAN_CITIES)

def check_addr_city_district(text: str) -> tuple:
    if not text or len(text) < 6:
        return True, ""
    matched_city = next((city for city in TAIWAN_CITIES if text.startswith(city)), None)
    if not matched_city:
        return False, f"ç„¡æ³•è­˜åˆ¥ç¸£å¸‚åç¨±ï¼ˆé–‹é ­ï¼š{text[:3]}ï¼‰"
    
    rest = text[len(matched_city):]
    district_char = next((ch for ch in rest if ch in ['å€', 'é„‰', 'é®']), None)
    if district_char is None:
        return True, ""

    level = _CITY_LEVEL.get(matched_city, '')
    if level == 'å¸‚' and district_char not in _DISTRICT_FOR_CITY:
        return False, f"å±¤ç´šéŒ¯èª¤ï¼šã€Œ{matched_city}ã€æ‡‰é…ã€Œå€ã€"
    if level == 'ç¸£' and district_char not in _DISTRICT_FOR_COUNTY:
        return False, f"å±¤ç´šéŒ¯èª¤ï¼šã€Œ{matched_city}ã€è¡Œæ”¿å€ä¸æ‡‰ç‚ºã€Œ{district_char}ã€"
    return True, ""

def fix_addr_prefix(text: str) -> tuple:
    if not text or len(text) < 3:
        return text, False
    if validate_addr_prefix(text):
        return text, False
    prefix = text[:3]
    best_match, best_score = None, 0.0
    for city in TAIWAN_CITIES:
        score = difflib.SequenceMatcher(None, prefix, city[:3]).ratio()
        if score > best_score:
            best_score, best_match = score, city
    if best_match and best_score >= 0.6:
        return best_match[:3] + text[3:], True
    return text, False

_ADDR_CHAR_MAP = {'è€‹': 'è‡º', 'è€¸': 'è‡º', 'å­¿': 'å­¸', 'å­½': 'å­¸', 'å£†': 'å­¸', 'è¦ƒ': 'å—'}

def fix_addr_post_process(text: str) -> str:
    if not text: return text
    text = apply_corrections(text.strip())
    for wrong, right in _ADDR_CHAR_MAP.items():
        text = text.replace(wrong, right)
    text = re.sub(r'(\d)\s+(\d)', r'\1\2', text)
    _ADDR_CJK = r'[é‡Œé„°è·¯æ®µå··å¼„è™Ÿè¡—å€å¸‚ç¸£é„‰é®æ‘]'
    text = re.sub(rf'({_ADDR_CJK})\s+(\d)', r'\1\2', text)
    text = re.sub(rf'(\d)\s+({_ADDR_CJK})', r'\1\2', text)
    text, _ = fix_addr_prefix(text)
    return text

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  OCR è™•ç†ç­–ç•¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def preprocess_for_ocr(img_gray: np.ndarray) -> list:
    imgs = []
    # ç­–ç•¥ 1 & 2 & 3
    for mode in ['normal', 'clahe', 'sharp']:
        processed = img_gray.copy()
        if mode == 'clahe':
            processed = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8)).apply(processed)
        elif mode == 'sharp':
            kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])
            processed = cv2.filter2D(processed, -1, kernel)
        
        big = cv2.resize(processed, None, fx=4, fy=4, interpolation=cv2.INTER_LANCZOS4)
        imgs.append(cv2.copyMakeBorder(big, 30, 30, 30, 30, cv2.BORDER_CONSTANT, value=255))
    return imgs

def ocr_with_best_result(ocr_model, img_gray: np.ndarray) -> tuple:
    strategy_names = ['åŸå§‹æ”¾å¤§', 'CLAHEå¢å¼·', 'éŠ³åŒ–']
    candidates = []
    for i, img in enumerate(preprocess_for_ocr(img_gray)):
        results = ocr_model.readtext(img, detail=1, paragraph=False)
        raw = "".join([res[1] for res in results if normalize(res[1]) not in ['åœ°å€', 'ä½å€']]).strip()
        processed = fix_addr_post_process(raw)
        candidates.append((processed, strategy_names[i]))

    def score_result(item):
        txt, _ = item
        s = 0
        if validate_addr_prefix(txt): s += 2
        ok, _ = check_addr_city_district(txt)
        if ok: s += 2
        if len(txt) > 5: s += 1
        return s
    return max(candidates, key=score_result) if candidates else ("", "ç„¡çµæœ")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  PDF è§£ææ ¸å¿ƒ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_addr_from_image_stream(page, ocr_model, debug_log: list):
    words = page.extract_words()
    addr_word = next((w for w in words if w['text'] in ['åœ°å€', 'ä½å€']), None)
    if not addr_word: return ""
    
    label = "ä½" if addr_word['text'] == 'ä½å€' else "åœ°"
    addr_imgs = [img for img in page.images if abs(img['top'] - addr_word['top']) < 5]
    if not addr_imgs: return ""

    try:
        raw_data = addr_imgs[0]['stream'].get_data()
        buf = np.frombuffer(raw_data, dtype=np.uint8)
        decoded = cv2.imdecode(buf, cv2.IMREAD_GRAYSCALE)
        if decoded is None: return ""
        addr_val, strat = ocr_with_best_result(ocr_model, decoded)
        debug_log.append(f"âœ… Stream æˆåŠŸ({strat}): {addr_val}")
        return f"{label} å€ {addr_val}"
    except Exception as e:
        debug_log.append(f"âŒ Stream å¤±æ•—: {e}")
        return ""

def ocr_addr_fallback(img_np, page, ocr_model, debug_log: list):
    h, w = img_np.shape[:2]
    sy, sx = h/page.height, w/page.width
    words = page.extract_words()
    addr_word = next((w for w in words if w['text'] in ['åœ°å€', 'ä½å€']), None)
    if not addr_word: return "[å®šä½å¤±æ•—]"

    next_words = [w for w in words if w['top'] > addr_word['bottom'] + 1]
    row_bottom = next_words[0]['top'] if next_words else addr_word['bottom'] + 20
    crop = img_np[max(0, int((addr_word['top']-2)*sy)):min(h, int((row_bottom+2)*sy)), int(175*sx):]
    
    gray = cv2.cvtColor(crop, cv2.COLOR_RGB2GRAY)
    addr_val, strat = ocr_with_best_result(ocr_model, gray)
    debug_log.append(f"âš ï¸ å‚™æ´æˆåŠŸ({strat}): {addr_val}")
    return f"{('ä½' if addr_word['text']=='ä½å€' else 'åœ°')} å€ {addr_val or '[ç„¡æ³•è¾¨è­˜]'}"

def detect_format(pdf):
    text = pdf.pages[0].extract_text() or ""
    if any(k in text for k in ["è¬„æœ¬ç¨®é¡ç¢¼", "åˆ—å°æ™‚é–“"]): return "è¬„æœ¬"
    if "ä¸€è¦½è¡¨" in text: return "ç¾¤ç’‡"
    if "ç¸£å¸‚" in normalize(text): return "å…‰ç‰¹"
    return "è¯å®‰"

# ... (å…¶é¤˜ process_ç¾¤ç’‡, process_è¬„æœ¬, process_è¡¨æ ¼å¼ é‚è¼¯ä¿æŒä¸è®Šï¼Œä½†èª¿ç”¨å„ªåŒ–å¾Œçš„ OCR å‡½æ•¸) ...
# [æ­¤è™•ç‚ºäº†ç°¡æ½”ï¼Œç•¥éé‡è¤‡çš„è¡¨æ ¼è§£æé‚è¼¯ï¼Œè«‹ç¢ºä¿èª¿ç”¨æ™‚ä½¿ç”¨ ocr_with_best_result]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Streamlit ä¸»ä»‹é¢
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="åœ°æ”¿æ–‡ä»¶é€è¦–å™¨", layout="wide")
ocr_model = load_ocr()

st.title("ğŸ  åœ°æ”¿æ–‡ä»¶é€è¦–å™¨")

uploaded_files = st.file_uploader("è«‹ä¸Šå‚³ PDF (æœ€å¤š 5 å€‹)", type="pdf", accept_multiple_files=True)

if uploaded_files:
    if len(uploaded_files) > 5:
        st.error("æœ€å¤š 5 å€‹æª”æ¡ˆ")
    elif st.button("ğŸš€ é–‹å§‹è™•ç†"):
        results = {}
        for uploaded_file in uploaded_files:
            with st.spinner(f"æ­£åœ¨è™•ç† {uploaded_file.name}..."):
                pdf_bytes = uploaded_file.read()
                # é€™è£¡èª¿ç”¨ä½ åŸæœ¬çš„ process_pdfï¼Œä½†å…§éƒ¨ç¢ºä¿ä½¿ç”¨å„ªåŒ–éçš„å‡½æ•¸
                # ç‚ºç¯€çœç©ºé–“ï¼Œè«‹å°‡ä½ åŸæœ‰çš„ process_ å‡½æ•¸ç¾¤çµ„æ”¾å›æ­¤è™•
                # è¨˜å¾— convert_from_bytes(pdf_bytes, dpi=300, poppler_path=POPPLER_PATH)
                st.success(f"{uploaded_file.name} è™•ç†å®Œæˆ")