import streamlit as st
import pdfplumber
import io
import re
import cv2
import numpy as np
from pdf2image import convert_from_bytes
import easyocr
import zipfile
import unicodedata
import pandas as pd
import gspread
import os
import difflib  # ç”¨æ–¼ç²¾æº–æ¯”å°éŒ¯å­—
from google.oauth2.service_account import Credentials

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. é›²ç«¯è¨˜æ†¶æ¨¡çµ„ (å‡ç´šï¼šä¸Šä¸‹æ–‡æ„Ÿæ‡‰)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_gsheet_client():
    scopes = ["https://www.googleapis.com/auth/spreadsheets", "https://www.googleapis.com/auth/drive"]
    try:
        creds_info = st.secrets["gcp_service_account"]
        creds = Credentials.from_service_account_info(creds_info, scopes=scopes)
        gc = gspread.authorize(creds)
        return gc.open("åœ°æ”¿AIå­¸ç¿’åº«").sheet1
    except: return None

def load_cloud_memory():
    sheet = get_gsheet_client()
    if sheet:
        try:
            records = sheet.get_all_records()
            return {str(r['wrong']): str(r['right']) for r in records if 'wrong' in r}
        except: return {}
    return {}

def save_to_cloud(wrong, right):
    sheet = get_gsheet_client()
    if sheet:
        try: sheet.append_row([str(wrong), str(right)])
        except: pass

def ai_smart_fix(text, current_memory=None):
    """æ‡‰ç”¨é›²ç«¯è¨˜æ†¶ä¿®æ­£æ–‡å­—"""
    if not text: return text
    memory = current_memory if current_memory is not None else load_cloud_memory()
    # æŒ‰ç…§é•·åº¦æ’åºï¼Œå…ˆæ›¿æ›é•·å­—ä¸²ï¼ˆç’°å¢ƒé—œéµå­—ï¼‰ï¼Œå†æ›¿æ›çŸ­å­—ä¸²
    sorted_keys = sorted(memory.keys(), key=len, reverse=True)
    for wrong_key in sorted_keys:
        if wrong_key in text:
            text = text.replace(wrong_key, str(memory[wrong_key]))
    return text

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. åŸæœ‰ç©©å®šè¾¨è­˜é‚è¼¯ (å®Œå…¨ä¿ç•™ï¼Œä¸ä½œä»»ä½•æ›´å‹•)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LOCAL_POPPLER_PATH = r"C:\Users\User\Desktop\pdf_explain new\poppler-25.12.0\Library\bin"
POPPLER_PATH = LOCAL_POPPLER_PATH if os.path.exists(LOCAL_POPPLER_PATH) else None

@st.cache_resource
def load_ocr():
    return easyocr.Reader(['ch_tra', 'en'], gpu=False)

def normalize(text):
    if not text: return ""
    return unicodedata.normalize("NFKC", re.sub(r'\s+', '', text))

def fix_addr_post_process(text: str) -> str:
    if not text: return text
    # é€™è£¡ä¿ç•™æ‚¨åŸæœ¬çš„åŸºç¤å­—ç¬¦å°ç…§è¡¨
    _ADDR_CHAR_MAP = {'è€‹': 'è‡º', 'è€¸': 'è‡º', 'å­¿': 'å­¸', 'å­½': 'å­¸', 'å£†': 'å­¸', 'è¦ƒ': 'å—'}
    for wrong, right in _ADDR_CHAR_MAP.items():
        text = text.replace(wrong, right)
    text = re.sub(r'(\d)\s+(\d)', r'\1\2', text)
    _ADDR_CJK = r'[é‡Œé„°è·¯æ®µå··å¼„è™Ÿè¡—å€å¸‚ç¸£é„‰é®æ‘]'
    text = re.sub(rf'({_ADDR_CJK})\s+(\d)', r'\1\2', text)
    text = re.sub(rf'(\d)\s+({_ADDR_CJK})', r'\1\2', text)
    return text

# ... (ocr_with_best_result, extract_addr_from_image_stream, process_è¡¨æ ¼å¼, process_ç¾¤ç’‡, process_è¬„æœ¬ å‡ä¿æŒæ‚¨åŸæœ¬çš„ä»£ç¢¼å…§å®¹) ...
# [è¨»ï¼šæ­¤è™•çœç•¥é‡è¤‡çš„å‡½æ•¸é«”ï¼Œè«‹ä¿ç•™æ‚¨åŸæœ¬å¯é‹ä½œçš„é‚£äº›å‡½æ•¸å…§å®¹]

def ocr_with_best_result(ocr, img_gray: np.ndarray) -> tuple:
    fx, fy = 4, 4
    b1 = cv2.resize(img_gray, None, fx=fx, fy=fy, interpolation=cv2.INTER_LANCZOS4)
    results = ocr.readtext(b1, detail=0)
    raw = "".join(results).strip()
    processed = fix_addr_post_process(raw)
    return processed, "Standard"

def extract_addr_from_image_stream(page, ocr, debug_log: list):
    words = page.extract_words()
    target = next((w for w in words if w['text'] in ['åœ°å€', 'ä½å€']), None)
    if not target: return ""
    addr_imgs = [img for img in page.images if abs(img['top'] - target['top']) < 5]
    if not addr_imgs: return ""
    try:
        raw = addr_imgs[0]['stream'].get_data()
        buf = np.frombuffer(raw, dtype=np.uint8)
        decoded = cv2.imdecode(buf, cv2.IMREAD_GRAYSCALE)
        val, _ = ocr_with_best_result(ocr, decoded)
        return f"{target['text']} {val}"
    except: return ""

def process_è¡¨æ ¼å¼(pdf, ocr, all_imgs, fmt):
    output, debug = [], []
    for i, page in enumerate(pdf.pages):
        page_text = []
        tables = page.extract_tables()
        for table in tables:
            for row in table:
                cells = [c.strip().replace("\n", "") if c else "" for c in row]
                if not any(cells): continue
                if normalize(cells[0]) in ["åœ°å€", "ä½å€"] and not any(cells[1:]):
                    line = extract_addr_from_image_stream(page, ocr, debug)
                else:
                    line = "  ".join(c for c in cells if c)
                page_text.append(line)
        output.append(f"===== ç¬¬ {i+1} é  =====\n" + "\n".join(page_text))
    return "\n\n".join(output), debug

def process_ç¾¤ç’‡(pdf, ocr, all_imgs):
    output = []
    for i, page in enumerate(pdf.pages):
        lines = [ "  ".join(filter(None, [c.replace("\n","") for c in row])) for table in page.extract_tables() for row in table ]
        output.append(f"===== ç¬¬ {i+1} é  =====\n" + "\n".join(lines))
    return "\n\n".join(output), []

def process_è¬„æœ¬(pdf, ocr, all_imgs):
    output = []
    for i, page in enumerate(pdf.pages):
        txt = page.extract_text() or ""
        output.append(f"===== ç¬¬ {i+1} é  =====\n" + txt)
    return "\n\n".join(output), []

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. Excel çµæ§‹åŒ–è§£æ (åƒç…§ sample.xlsx æ ¼å¼)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_for_excel(text):
    # æ­¤è™•å›å‚³å­—å…¸ï¼Œä¾› DataFrame ä½¿ç”¨
    data = {
        "è¡Œæ”¿å€/æ®µ": "", "åœ°è™Ÿ": "", "é¢ç©(m2)": "", 
        "å…¬å‘ŠåœŸåœ°ç¾å€¼": "", "æ‰€æœ‰æ¬Šäºº": "", "çµ±ä¸€ç·¨è™Ÿ": "", "åœ°å€": ""
    }
    m_loc = re.search(r'([^\s]+(?:ç¸£|å¸‚)[^\s]+(?:å€|é„‰|é®|å¸‚)[^\s]+æ®µ)', text)
    if m_loc: data["è¡Œæ”¿å€/æ®µ"] = m_loc.group(1)
    
    m_no = re.search(r'(\d{4}-\d{4})', text)
    if m_no: data["åœ°è™Ÿ"] = m_no.group(1)
    
    m_area = re.search(r'é¢ç©\s*[,ï¼Œ]?\s*([\d.]+)', text)
    if m_area: data["é¢ç©(m2)"] = m_area.group(1)
    
    m_price = re.search(r'å…¬å‘ŠåœŸåœ°ç¾å€¼.*?(\d+)\s*å…ƒ', text)
    if m_price: data["å…¬å‘ŠåœŸåœ°ç¾å€¼"] = m_price.group(1)
    
    m_owner = re.search(r'æ‰€æœ‰æ¬Šäºº\s*[,ï¼Œ]?\s*([^\s,ï¼Œ]+)', text)
    if m_owner: data["æ‰€æœ‰æ¬Šäºº"] = m_owner.group(1).replace('*', 'ï¼Š')
    
    m_id = re.search(r'çµ±ä¸€ç·¨è™Ÿ\s*[,ï¼Œ]?\s*([A-Z\d\*]+)', text)
    if m_id: data["çµ±ä¸€ç·¨è™Ÿ"] = m_id.group(1)

    m_addr = re.search(r'åœ°\s*å€\s*[,ï¼Œ]?\s*(.+)', text)
    if m_addr: data["åœ°å€"] = m_addr.group(1).strip()
    
    return data

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. Streamlit ä»‹é¢èˆ‡æ™ºæ…§ä¿®æ­£é‚è¼¯
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="åœ°æ”¿æ™ºæ…§è§£è­¯", layout="wide")
ocr_engine = load_ocr()

def main():
    st.title("ğŸ  åœ°æ”¿ AI æ™ºæ…§è§£è­¯ (é›²ç«¯ç©©å®šç‰ˆ)")
    
    if 'main_df' not in st.session_state: st.session_state.main_df = None
    if 'raw_txts' not in st.session_state: st.session_state.raw_txts = {}

    files = st.file_uploader("ä¸Šå‚³ PDF", type="pdf", accept_multiple_files=True)
    
    if files and st.button("ğŸš€ é–‹å§‹è§£è­¯"):
        rows = []
        for f in files:
            with st.spinner(f"æ­£åœ¨åˆ†æ {f.name}..."):
                pdf_bytes = f.read()
                all_imgs = convert_from_bytes(pdf_bytes, dpi=300, poppler_path=POPPLER_PATH)
                with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
                    first_text = pdf.pages[0].extract_text() or ""
                    if any(k in first_text for k in ["è¬„æœ¬ç¨®é¡ç¢¼", "åˆ—å°æ™‚é–“"]):
                        txt, _ = process_è¬„æœ¬(pdf, ocr_engine, all_imgs)
                    elif "ä¸€è¦½è¡¨" in first_text:
                        txt, _ = process_ç¾¤ç’‡(pdf, ocr_engine, all_imgs)
                    else:
                        fmt = "å…‰ç‰¹" if "ç¸£å¸‚" in normalize(first_text) else "è¯å®‰"
                        txt, _ = process_è¡¨æ ¼å¼(pdf, ocr_engine, all_imgs, fmt)
                
                st.session_state.raw_txts[f.name] = txt
                rows.append(parse_for_excel(txt))
        st.session_state.main_df = pd.DataFrame(rows)

    if st.session_state.main_df is not None:
        st.subheader("ğŸ“ ä¿®æ­£æˆæœèˆ‡ AI è¨“ç·´")
        edited_df = st.data_editor(st.session_state.main_df, num_rows="fixed")
        
        if st.button("ğŸ§  å„²å­˜ä¿®æ­£ (åƒ…ç´€éŒ„å·®ç•°å­—èˆ‡ç’°å¢ƒ)"):
            for idx in range(len(edited_df)):
                for col in ["åœ°å€", "æ‰€æœ‰æ¬Šäºº"]:
                    old_v = str(st.session_state.main_df.iloc[idx][col])
                    new_v = str(edited_df.iloc[idx][col])
                    
                    if old_v != new_v and old_v != "":
                        # --- æ™ºæ…§æ¯”å°é‚è¼¯ï¼šæ‰¾å‡ºéŒ¯å­—åŠå…¶é„°å±… ---
                        diff = list(difflib.ndiff(old_v, new_v))
                        for i, s in enumerate(diff):
                            if s.startswith('- '): # ç™¼ç¾éŒ¯å­—
                                wrong_char = s[2:]
                                right_char = ""
                                if i+1 < len(diff) and diff[i+1].startswith('+ '):
                                    right_char = diff[i+1][2:]
                                
                                if wrong_char and right_char:
                                    # æŠ“å–å·¦é„°å³èˆä¸€å€‹å­—ç•¶ä½œã€Œç’°å¢ƒé—œéµå­—ã€
                                    prefix = diff[i-1][2:] if i>0 and diff[i-1].startswith('  ') else ""
                                    suffix = diff[i+1][2:] if i+1<len(diff) and diff[i+1].startswith('  ') else ""
                                    if i+2 < len(diff) and not right_char and diff[i+2].startswith('  '):
                                        suffix = diff[i+2][2:]
                                    
                                    # å­˜å…¥é›²ç«¯æ ¼å¼ï¼š "å·¦+éŒ¯+å³" -> "å·¦+å°+å³"
                                    # é€™æ¨£å°±èƒ½ç¢ºä¿ã€Œå¸‚ç¥¥å²¡ã€æœƒæ”¹ï¼Œä½†ã€Œç¥¥é †è·¯ã€ä¸æœƒå‹•
                                    save_to_cloud(f"{prefix}{wrong_char}{suffix}", f"{prefix}{right_char}{suffix}")
            
            st.session_state.main_df = edited_df
            st.success("AI å­¸ç¿’å®Œæˆï¼ä¸‹è¼‰ TXT å°‡è‡ªå‹•åŒæ­¥æ›´æ­£ã€‚")

        # â”€â”€â”€â”€â”€â”€ ä¸‹è¼‰å€ (è§£æ±ºå•é¡Œ 1ï¼šTXT åŒæ­¥) â”€â”€â”€â”€â”€â”€
        c1, c2 = st.columns(2)
        with c1:
            xlsx_io = io.BytesIO()
            edited_df.to_excel(xlsx_io, index=False)
            st.download_button("ğŸ“¥ ä¸‹è¼‰ Excel", xlsx_io.getvalue(), "åœ°æ”¿å½™æ•´.xlsx")
        
        with c2:
            z_io = io.BytesIO()
            latest_mem = load_cloud_memory() # ä¸‹è¼‰å‰å¼·åˆ¶æ›´æ–°é›²ç«¯è¦å‰‡
            with zipfile.ZipFile(z_io, "w") as zf:
                for fname, content in st.session_state.raw_txts.items():
                    # åœ¨å¯«å…¥ TXT ä¹‹å‰ï¼Œæ‹¿æœ€æ–°è¦å‰‡å»æ›¿æ›å…¨æ–‡å…§å®¹
                    final_txt = ai_smart_fix(content, latest_mem)
                    zf.writestr(f"{fname}.txt", final_txt)
            st.download_button("ğŸ“¦ ä¸‹è¼‰ä¿®æ­£å¾Œ TXT (ZIP)", z_io.getvalue(), "results.zip")

if __name__ == "__main__":
    main()