import streamlit as st
import pdfplumber
import io
import re
import cv2
import numpy as np
from pdf2image import convert_from_bytes
import easyocr
import zipfile
import unicodedata
import json
import os
import difflib

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ç’°å¢ƒé©æ‡‰èˆ‡è³‡æºè¼‰å…¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# è‡ªå‹•åµæ¸¬ Poppler è·¯å¾‘
LOCAL_POPPLER_PATH = r"C:\Users\User\Desktop\pdf_explain new\poppler-25.12.0\Library\bin"
POPPLER_PATH = LOCAL_POPPLER_PATH if os.path.exists(LOCAL_POPPLER_PATH) else None
CORRECTIONS_FILE = "addr_corrections.json"

@st.cache_resource
def load_ocr():
    # é›²ç«¯ç’°å¢ƒé€šå¸¸æ²’æœ‰ GPUï¼Œè¨­å®šç‚º False ä»¥å…å ±éŒ¯
    return easyocr.Reader(['ch_tra', 'en'], gpu=False)

def normalize(text):
    if not text: return ""
    return unicodedata.normalize("NFKC", re.sub(r'\s+', '', text))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. åœ°å€é©—è­‰èˆ‡æ ¡æ­£é‚è¼¯
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

TAIWAN_CITIES = [
    'è‡ºåŒ—å¸‚', 'æ–°åŒ—å¸‚', 'æ¡ƒåœ’å¸‚', 'è‡ºä¸­å¸‚', 'è‡ºå—å¸‚', 'é«˜é›„å¸‚',
    'åŸºéš†å¸‚', 'æ–°ç«¹å¸‚', 'å˜‰ç¾©å¸‚', 'æ–°ç«¹ç¸£', 'è‹—æ —ç¸£', 'å½°åŒ–ç¸£', 
    'å—æŠ•ç¸£', 'é›²æ—ç¸£', 'å˜‰ç¾©ç¸£', 'å±æ±ç¸£', 'å®œè˜­ç¸£', 'èŠ±è“®ç¸£', 
    'è‡ºæ±ç¸£', 'æ¾æ¹–ç¸£', 'é‡‘é–€ç¸£', 'é€£æ±Ÿç¸£', 'å°åŒ—å¸‚', 'å°ä¸­å¸‚', 
    'å°å—å¸‚', 'å°æ±ç¸£', 'å°åŒ—ç¸£', 'æ¡ƒåœ’ç¸£', 'å°ä¸­ç¸£', 'å°å—ç¸£', 'é«˜é›„ç¸£',
]

_CITY_LEVEL = {c: ('å¸‚' if c.endswith('å¸‚') else 'ç¸£') for c in TAIWAN_CITIES}
_DISTRICT_FOR_CITY = ['å€']
_DISTRICT_FOR_COUNTY = ['å€', 'é„‰', 'é®', 'å¸‚']

def load_corrections() -> dict:
    if os.path.exists(CORRECTIONS_FILE):
        try:
            with open(CORRECTIONS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except: return {}
    return {}

def save_correction(wrong: str, right: str):
    corrections = load_corrections()
    corrections[wrong.strip()] = right.strip()
    with open(CORRECTIONS_FILE, 'w', encoding='utf-8') as f:
        json.dump(corrections, f, ensure_ascii=False, indent=2)

def apply_corrections(text: str) -> str:
    return load_corrections().get(text.strip(), text)

def validate_addr_prefix(text: str) -> bool:
    return any(text.startswith(city) for city in TAIWAN_CITIES)

def check_addr_city_district(text: str) -> tuple:
    if not text or len(text) < 6: return True, ""
    matched_city = next((city for city in TAIWAN_CITIES if text.startswith(city)), None)
    if not matched_city: return False, f"ç„¡æ³•è­˜åˆ¥ç¸£å¸‚åç¨±ï¼ˆ{text[:3]}ï¼‰"
    
    rest = text[len(matched_city):]
    district_char = next((ch for ch in rest if ch in ['å€', 'é„‰', 'é®']), None)
    if not district_char: return True, ""

    level = _CITY_LEVEL.get(matched_city, '')
    if level == 'å¸‚' and district_char not in _DISTRICT_FOR_CITY:
        return False, f"å±¤ç´šéŒ¯èª¤ï¼šã€Œ{matched_city}ã€é…ã€Œ{district_char}ã€ï¼ˆæ‡‰ç‚ºå€ï¼‰"
    if level == 'ç¸£' and district_char not in _DISTRICT_FOR_COUNTY:
        return False, f"å±¤ç´šéŒ¯èª¤ï¼šã€Œ{matched_city}ã€é…ã€Œ{district_char}ã€"
    return True, ""

def fix_addr_prefix(text: str) -> tuple:
    if not text or len(text) < 3 or validate_addr_prefix(text):
        return text, False
    prefix = text[:3]
    best_match, best_score = None, 0.0
    for city in TAIWAN_CITIES:
        score = difflib.SequenceMatcher(None, prefix, city[:3]).ratio()
        if score > best_score:
            best_score, best_match = score, city
    if best_match and best_score >= 0.6:
        return best_match[:3] + text[3:], True
    return text, False

_ADDR_CHAR_MAP = {'è€‹': 'è‡º', 'è€¸': 'è‡º', 'å­¿': 'å­¸', 'å­½': 'å­¸', 'å£†': 'å­¸', 'è¦ƒ': 'å—'}

def fix_addr_post_process(text: str) -> str:
    if not text: return text
    text = apply_corrections(text.strip())
    for wrong, right in _ADDR_CHAR_MAP.items():
        text = text.replace(wrong, right)
    text = re.sub(r'(\d)\s+(\d)', r'\1\2', text)
    _ADDR_CJK = r'[é‡Œé„°è·¯æ®µå··å¼„è™Ÿè¡—å€å¸‚ç¸£é„‰é®æ‘]'
    text = re.sub(rf'({_ADDR_CJK})\s+(\d)', r'\1\2', text)
    text = re.sub(rf'(\d)\s+({_ADDR_CJK})', r'\1\2', text)
    text, _ = fix_addr_prefix(text)
    return text

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. OCR æ ¸å¿ƒèˆ‡å¤šç­–ç•¥è¾¨è­˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def preprocess_for_ocr(img_gray: np.ndarray) -> list:
    imgs = []
    # èª¿æ•´å€ç‡ç‚º 4 å€ï¼Œå…¼é¡§é›²ç«¯æ•ˆèƒ½èˆ‡æº–ç¢ºç‡
    big_size = (None, None) 
    fx, fy = 4, 4
    
    # ç­–ç•¥ 1: åŸå§‹æ”¾å¤§
    b1 = cv2.resize(img_gray, None, fx=fx, fy=fy, interpolation=cv2.INTER_LANCZOS4)
    imgs.append(cv2.copyMakeBorder(b1, 30, 30, 30, 30, cv2.BORDER_CONSTANT, value=255))
    
    # ç­–ç•¥ 2: CLAHE å¢å¼·
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8)).apply(img_gray)
    b2 = cv2.resize(clahe, None, fx=fx, fy=fy, interpolation=cv2.INTER_LANCZOS4)
    imgs.append(cv2.copyMakeBorder(b2, 30, 30, 30, 30, cv2.BORDER_CONSTANT, value=255))
    
    return imgs

def ocr_with_best_result(ocr, img_gray: np.ndarray) -> tuple:
    strategies = ['åŸå§‹', 'CLAHE']
    candidates = []
    for i, img in enumerate(preprocess_for_ocr(img_gray)):
        results = ocr.readtext(img, detail=1, paragraph=False)
        raw = "".join([res[1] for res in results if normalize(res[1]) not in ['åœ°å€', 'ä½å€']]).strip()
        processed = fix_addr_post_process(raw)
        candidates.append((processed, strategies[i]))

    def score(item):
        t, _ = item
        s = 0
        if validate_addr_prefix(t): s += 2
        ok, _ = check_addr_city_district(t)
        if ok: s += 2
        if len(t) > 5: s += 1
        return s
    return max(candidates, key=score) if candidates else ("", "ç„¡çµæœ")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. æ–‡ä»¶è§£æé‚è¼¯ (è¡¨æ ¼ã€é›»å‚³ã€è¬„æœ¬)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def clean_watermark(text):
    lines = text.split("\n")
    cleaned = []
    watermark_chars = set("è‡ºå—å¸‚æ–°åŒ–åœ°æ”¿äº‹å‹™æ‰€")
    for line in lines:
        s = line.strip()
        if s == "H0" or (len(s) == 1 and s in watermark_chars): continue
        line = re.sub(r'\s+[è‡ºå—å¸‚æ–°åŒ–åœ°æ”¿äº‹å‹™æ‰€]{1,2}\s*$', '', line)
        line = re.sub(r'è‡º(ä¸€èˆ¬è¾²æ¥­å€|éƒ½å¸‚ç™¼å±•å€|è¾²æ¥­å€)', r'\1', line)
        cleaned.append(line)
    return "\n".join(cleaned)

def extract_addr_from_image_stream(page, ocr, debug_log: list):
    words = page.extract_words()
    target = next((w for w in words if w['text'] in ['åœ°å€', 'ä½å€']), None)
    if not target: return ""
    
    label = "ä½" if target['text'] == 'ä½å€' else "åœ°"
    addr_imgs = [img for img in page.images if abs(img['top'] - target['top']) < 5]
    if not addr_imgs: return ""

    try:
        raw = addr_imgs[0]['stream'].get_data()
        buf = np.frombuffer(raw, dtype=np.uint8)
        decoded = cv2.imdecode(buf, cv2.IMREAD_GRAYSCALE)
        val, strat = ocr_with_best_result(ocr, decoded)
        debug_log.append(f"âœ… StreamæˆåŠŸ({strat}): {val}")
        return f"{label} å€ {val}"
    except: return ""

def ocr_addr_fallback(img_np, page, ocr, debug_log: list):
    h, w = img_np.shape[:2]
    sy, sx = h/page.height, w/page.width
    words = page.extract_words()
    target = next((w for w in words if w['text'] in ['åœ°å€', 'ä½å€']), None)
    if not target: return "[å®šä½å¤±æ•—]"

    next_w = [w for w in words if w['top'] > target['bottom'] + 1]
    bottom = next_w[0]['top'] if next_w else target['bottom'] + 20
    crop = img_np[max(0, int((target['top']-2)*sy)):min(h, int((bottom+2)*sy)), int(175*sx):]
    
    gray = cv2.cvtColor(crop, cv2.COLOR_RGB2GRAY)
    val, strat = ocr_with_best_result(ocr, gray)
    debug_log.append(f"âš ï¸ å‚™æ´æˆåŠŸ({strat}): {val}")
    return f"{('ä½' if target['text']=='ä½å€' else 'åœ°')} å€ {val or '[ç„¡æ³•è¾¨è­˜]'}"

# æ ¼å¼è™•ç†å‡½æ•¸ç¾¤
def process_è¡¨æ ¼å¼(pdf, ocr, all_imgs, fmt):
    output, debug = [], []
    for i, page in enumerate(pdf.pages):
        page_text = []
        tables = page.extract_tables()
        if tables:
            for table in tables:
                for row in table:
                    cells = [c.strip().replace("\n", "") if c else "" for c in row]
                    if not any(cells): continue
                    
                    # åˆ¤æ–·æ˜¯å¦ç‚ºç©ºåœ°å€åˆ—
                    is_addr = normalize(cells[0]) in ["åœ°å€", "ä½å€"]
                    has_content = any(c.strip() for c in cells[1:])
                    
                    if is_addr and not has_content:
                        line = extract_addr_from_image_stream(page, ocr, debug)
                        if not line:
                            line = ocr_addr_fallback(np.array(all_imgs[i]), page, ocr, debug)
                    else:
                        line = "  ".join(c for c in cells if c)
                    page_text.append(line)
        output.append(f"===== ç¬¬ {i+1} é  =====\n" + "\n".join(page_text))
    return "\n\n".join(output), debug

def process_ç¾¤ç’‡(pdf, ocr, all_imgs):
    output = []
    for i, page in enumerate(pdf.pages):
        lines = []
        tables = page.extract_tables()
        for table in tables:
            for row in table:
                cells = [c.strip().replace("\n", "") if c else "" for c in row]
                if not any(cells) or any(x in "".join(cells) for x in ["ä¸€è¦½è¡¨", "åˆ—å°"]): continue
                if len(cells) >= 2 and normalize(cells[0]) in ["åœ°å€", "ä½å€"]:
                    lines.append(f"åœ°  å€  {cells[1]}")
                else:
                    lines.append("  ".join(c for c in cells if c))
        output.append(f"===== ç¬¬ {i+1} é  =====\n" + "\n".join(lines))
    return "\n\n".join(output), []

def process_è¬„æœ¬(pdf, ocr, all_imgs):
    output, debug = [], []
    for i, page in enumerate(pdf.pages):
        raw = clean_watermark(page.extract_text() or "")
        lines = raw.split("\n")
        res_lines = []
        for j, line in enumerate(lines):
            res_lines.append(line)
            if "æ‰€æœ‰æ¬Šäºº" in line:
                nxt = lines[j+1].strip() if j+1 < len(lines) else ""
                if "å€" not in nxt and "çµ±ä¸€ç·¨è™Ÿ" not in nxt:
                    img_np = np.array(all_imgs[i])
                    h, w = img_np.shape[:2]
                    scale = h/page.height
                    words = page.extract_words()
                    y = next((wd["top"] for wd in words if "æ‰€æœ‰æ¬Šäºº" in wd["text"]), None)
                    if y:
                        crop = img_np[int((y+10)*scale):int((y+70)*scale), :int(w*0.85)]
                        val, strat = ocr_with_best_result(ocr, cv2.cvtColor(crop, cv2.COLOR_RGB2GRAY))
                        if val: res_lines.append(f" ä½  å€ï¼š{val.replace('ä½å€ï¼š','')}")
        output.append(f"===== ç¬¬ {i+1} é  =====\n" + "\n".join(res_lines))
    return "\n\n".join(output), debug

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. ä¸»å…¥å£èˆ‡ Streamlit UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.set_page_config(page_title="åœ°æ”¿æ–‡ä»¶é€è¦–å™¨", layout="wide")
ocr = load_ocr()

def main():
    st.title("ğŸ  åœ°æ”¿æ–‡ä»¶é€è¦–å™¨")
    
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        show_debug = st.checkbox("é¡¯ç¤ºé™¤éŒ¯è³‡è¨Š")
        if st.button("ğŸ§¹ æ¸…é™¤æš«å­˜"):
            st.cache_resource.clear()
            st.rerun()

    files = st.file_uploader("ä¸Šå‚³ PDF", type="pdf", accept_multiple_files=True)
    
    if files and st.button("ğŸš€ é–‹å§‹è™•ç†"):
        all_results = {}
        for f in files[:5]:
            with st.spinner(f"æ­£åœ¨è™•ç† {f.name}..."):
                pdf_bytes = f.read()
                all_imgs = convert_from_bytes(pdf_bytes, dpi=300, poppler_path=POPPLER_PATH)
                
                with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
                    text = pdf.pages[0].extract_text() or ""
                    if any(k in text for k in ["è¬„æœ¬ç¨®é¡ç¢¼", "åˆ—å°æ™‚é–“"]):
                        txt, dbg = process_è¬„æœ¬(pdf, ocr, all_imgs)
                    elif "ä¸€è¦½è¡¨" in text:
                        txt, dbg = process_ç¾¤ç’‡(pdf, ocr, all_imgs)
                    else:
                        fmt = "å…‰ç‰¹" if "ç¸£å¸‚" in normalize(text) else "è¯å®‰"
                        txt, dbg = process_è¡¨æ ¼å¼(pdf, ocr, all_imgs, fmt)
                
                all_results[f.name] = txt
                st.success(f"âœ… {f.name} å®Œæˆ")
                
                if show_debug:
                    with st.expander(f"ğŸ” {f.name} é™¤éŒ¯æ—¥èªŒ"):
                        for d in dbg: st.text(d)
                
                st.text_area(f"é è¦½: {f.name}", txt, height=200)
                st.download_button(f"ä¸‹è¼‰ {f.name}.txt", txt, f"{f.name}.txt")

        if len(all_results) > 1:
            z_buf = io.BytesIO()
            with zipfile.ZipFile(z_buf, "w") as zf:
                for n, c in all_results.items(): zf.writestr(f"{n}.txt", c)
            st.download_button("ğŸ“¦ ä¸‹è¼‰å…¨éƒ¨ (ZIP)", z_buf.getvalue(), "results.zip")

if __name__ == "__main__":
    main()